{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copyright 2022 University of Luxembourg\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "> you may not use this file except in compliance with the License.  \n",
    "> You may obtain a copy of the License at  \n",
    ">\n",
    ">    https://www.apache.org/licenses/LICENSE-2.0\n",
    ">\n",
    "> Unless required by applicable law or agreed to in writing, software  \n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "> See the License for the specific language governing permissions and  \n",
    "> limitations under the License.  \n",
    ">\n",
    "***\n",
    "\n",
    "Author: AndrÃ© Stemper (andre.stemper@uni.lu)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *%run dataset.ipynb* to include this notebook into another notebook.  \n",
    "\n",
    "Possible datasets:  \n",
    "[\"2022.02.09\", \"2022.02.16\", \"2022.02.25\", \"2022.03.09\", \"2022.03.16\", \"2022.03.23\", \"2022.04.06\", \"2022.05.13\",  \n",
    " \"2022.05.18\", \"2022.05.20\", \"2022.05.30\", \"2022.06.01\", \"2022.06.03\", \"2022.06.08\", \"2022.06.15\", \"2022.06.22\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, json, math, copy\n",
    "import datetime as _datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTest(object):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"\"\n",
    "\n",
    "    def test(self, dataset):\n",
    "        return False\n",
    "\n",
    "    def run(self, dataset):\n",
    "        try:\n",
    "            return self.test(dataset)\n",
    "        except Exception as e:\n",
    "            raise(Exception(\"Test failed to execute: {}\".format(e)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for checking for uptime monotonicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTest_UptimeMonotonicity(DatasetTest):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"uptime is monotonic\"\n",
    "\n",
    "    def test(self, dataset):\n",
    "        return (dataset.dataframe['uptimeInS']).astype('float64').is_monotonic \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for checking for uniform sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetTest_IndexUniformity(DatasetTest):\n",
    "\n",
    "    epsilon=_datetime.timedelta(milliseconds=300)\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"index is uniform (epsilon={})\".format(self.epsilon)\n",
    "\n",
    "    def test(self, dataset):\n",
    "        delta = (dataset.dataframe.index[1:]-dataset.dataframe.index[0:-1]).fillna(0)\n",
    "        deltadelta = delta[1:] - delta[0:-1]\n",
    "        return not (deltadelta > self.epsilon).any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sample period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTest_SamplingPeriod(DatasetTest):\n",
    "        def __init__(self, period=5.0, epsilon=0.2):\n",
    "            self.period = period\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "        @property\n",
    "        def name(self):\n",
    "            return \"sampling period close to target period\"\n",
    "\n",
    "        def test(self, dataset):\n",
    "            p,s = dataset.period\n",
    "            return abs(p-self.period) < self.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):    \n",
    "    def __init__(self, name, verbose=False, separate_angle_sin=True, datasets_folder=['.', 'datasets']):\n",
    "        self.dataset_name = name  \n",
    "        self.verbose = verbose\n",
    "        self.separate_angle_sin = separate_angle_sin\n",
    "        #\n",
    "        self.__meta = {}    \n",
    "        self.__dataframe = None        # active dataframe\n",
    "        self.__loaded_dataframe = None # original unscaled dataframe\n",
    "        self.__partial_dataframes={}   # partial dataframes merged into loaded dataframe\n",
    "        #\n",
    "        self._applied_scale = (0, 1) # a+b*x\n",
    "        #\n",
    "        self.contains_telemetry_data = False\n",
    "        self.contains_table_data = False\n",
    "        self.contains_sin_of_angle = False\n",
    "        self.contains_room_temperature_data = False\n",
    "        # \n",
    "        self.__load(name, datasets_folder=datasets_folder)\n",
    "\n",
    "    @property\n",
    "    def anomalies(self):\n",
    "        \"\"\" list of anomalies \"\"\"\n",
    "        try:\n",
    "            return self.__meta[\"anomalies\"][\"data\"]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    @property\n",
    "    def comment(self):\n",
    "        try:\n",
    "            return str(self.__meta[\"comment\"])\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    @property\n",
    "    def dataframe(self):\n",
    "        \"\"\" pandas dataframe \"\"\"\n",
    "        return self.__dataframe\n",
    "\n",
    "    @property\n",
    "    def is_valid(self):\n",
    "        return all(self.test(human_readable=False).values())\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return len(self.__dataframe)\n",
    "\n",
    "    @property\n",
    "    def meta(self):\n",
    "        \"\"\" meta data \"\"\"\n",
    "        return self.__meta\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.dataset_name\n",
    "\n",
    "    @property\n",
    "    def period(self):\n",
    "        delta = (self.__dataframe.index[1:]-self.__dataframe.index[0:-1]).fillna(0)\n",
    "        return (delta.mean().total_seconds(), delta.std().total_seconds())\n",
    "\n",
    "    @property\n",
    "    def ranges(self):\n",
    "        \"\"\" dict of ranges \"\"\"\n",
    "        try:\n",
    "            return self.__meta[\"ranges\"]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    @property \n",
    "    def scale_factor(self):\n",
    "        return self._applied_scale\n",
    "\n",
    "    def get_partial_dataframe(self, name):\n",
    "        try:\n",
    "            return self.__partial_dataframes[name]\n",
    "        except:\n",
    "            raise KeyError\n",
    "\n",
    "    def plot(self, columns=['temp_0', 'temp_1', 'temp_2', 'temp_3', 'temp_4', 'temp_5', 'temp_6', 'temp_7', 'temp_8', 'angle', 'sin_of_angle'], loc='best', column=None, more_columns=None, not_columns=None,  **kwargs):                           \n",
    "        \"\"\" plot data \"\"\"\n",
    "        if not column is None:\n",
    "            columns=[column]\n",
    "        if type(more_columns) == str:\n",
    "            more_columns = list(more_columns)\n",
    "        if not more_columns is None:\n",
    "            columns = columns + more_columns\n",
    "        if not not_columns is None:\n",
    "            try:\n",
    "                for v in not_columns:\n",
    "                    columns.remove(v)\n",
    "            except:\n",
    "                pass\n",
    "        if not self.contains_table_data:\n",
    "            try:\n",
    "                columns.remove('angle')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                columns.remove('sin_of_angle')\n",
    "            except:\n",
    "                pass\n",
    "        if not self.separate_angle_sin:   \n",
    "            try:\n",
    "                columns.remove('sin_of_angle')\n",
    "            except:\n",
    "                pass\n",
    "        if not self.contains_room_temperature_data:\n",
    "            try:\n",
    "                columns.remove('room_temperature')\n",
    "            except:\n",
    "                pass\n",
    "        self.__dataframe.plot(y=columns, **kwargs).legend(loc=loc)\n",
    "        return self\n",
    "\n",
    "    def plot_anomalies(self, ax=None, **kwargs):\n",
    "        \"\"\" plot anomalies \"\"\"\n",
    "        for anomaly in self.anomalies:\n",
    "            # start = _datetime.datetime.strptime(anomaly['start'], '%d.%m.%Y %H:%M:%S.%f')\n",
    "            start = anomaly['start']\n",
    "            try:\n",
    "                start = start + self.__meta['rebase_offset']\n",
    "            except KeyError:                \n",
    "                pass\n",
    "            if ax is None:\n",
    "                ax = plt\n",
    "            ax.axvline(start, linestyle='dashed', color=(1, 0, 0, 0.5), **kwargs)     \n",
    "        return self\n",
    "\n",
    "    def plot_ranges(self, ranges=None, alpha=1.0, ax=None, **kwargs):\n",
    "        if ranges is None:\n",
    "            ranges = self.ranges.keys()\n",
    "\n",
    "        __parse_time = lambda x: _datetime.datetime.strptime(x, '%d.%m.%Y %H:%M:%S.%f')\n",
    "\n",
    "        for r in ranges:\n",
    "            rr = self.ranges[r]\n",
    "            try:\n",
    "                color = rr['color']\n",
    "            except:\n",
    "                color = None\n",
    "            try:\n",
    "                metaalpha = rr['alpha']\n",
    "            except:\n",
    "                metaalpha = 0.2\n",
    "            try:\n",
    "                start = __parse_time(rr['start'])\n",
    "            except:\n",
    "                start = self.__dataframe.index[0]\n",
    "            try:\n",
    "                stop = __parse_time(rr['stop'])\n",
    "            except:\n",
    "                stop = self.__dataframe.index[-1]\n",
    "            if ax is None:\n",
    "                ax = plt\n",
    "            ax.axvspan(start, stop, color=color, alpha=alpha*metaalpha, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def plot_time(self, **kwargs):\n",
    "        \"\"\" plot time \"\"\"\n",
    "        plt.plot(self.__dataframe.index.values[0:], **kwargs)        \n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self.__dataframe = self.__loaded_dataframe \n",
    "        self._applied_scale = (0, 1) \n",
    "        return self\n",
    "\n",
    "    def save_dataframe_as_csv(self, filename='dataframe.csv', **kwargs):  \n",
    "        \"\"\" save dataframe \"\"\"\n",
    "        if not self.__dataframe is None:  \n",
    "            self.__dataframe.to_csv(filename, **kwargs)\n",
    "        return self    \n",
    "\n",
    "    def overwrite_dataframe_from_csv(self, filename='dataframe.csv', **kwargs):  \n",
    "        \"\"\" load dataframe keeping current index \"\"\"\n",
    "        if not self.__dataframe is None:  \n",
    "            index = self.__dataframe.index\n",
    "            self.__dataframe = pd.read_csv(filename, **kwargs)\n",
    "            self.__dataframe.index = index \n",
    "        return self    \n",
    "\n",
    "    def test(self, human_readable=True, tests=None):\n",
    "        \"\"\" run tests on dataset and format results \"\"\"\n",
    "        # run tests\n",
    "        if tests is None:\n",
    "            tests = [\n",
    "                DatasetTest_UptimeMonotonicity(),\n",
    "                DatasetTest_IndexUniformity()\n",
    "                ]\n",
    "        results = {}\n",
    "        for test in tests:\n",
    "            try:\n",
    "                results[test.name] = test.run(self)\n",
    "            except Exception as e:\n",
    "                results[test.name] = e\n",
    "        # format \n",
    "        if human_readable:\n",
    "            readable_results = []\n",
    "            for name, result in results.items():\n",
    "                if type(result) is bool:\n",
    "                    readable_results.append(\" {} checking if {}: {}\".format(\"+\" if result else \"-\", name, \"passed.\" if result else \"failed.\"))\n",
    "                else:\n",
    "                    readable_results.append(\" ! checking if {} caused an exception: {}\".format(name, e))\n",
    "            return '\\r\\n'.join(readable_results)\n",
    "        return results\n",
    "\n",
    "    def test_all_ranges(self):\n",
    "        \"\"\" run tests over all known ranges \"\"\"\n",
    "        results=[]\n",
    "        for r in dataset.ranges.keys():\n",
    "            results.append(\"Testing range '{}'\".format(r))\n",
    "            results.append(dataset[r].test())\n",
    "        return '\\r\\n'.join(results) \n",
    "\n",
    "    def extract_by_date(self, start=None, stop=None):\n",
    "        \"\"\" returns a dataset between start and end dates. \n",
    "             - if start is None the beginning of this dataset will be used\n",
    "             - if end is None the end of this dataset will be used \n",
    "            dates are in the format d.m.Y H:M:S.f\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_start_index = list(self.__dataframe.index >= _datetime.datetime.strptime(start, '%d.%m.%Y %H:%M:%S.%f')).index(True)                        \n",
    "        except Exception as e:\n",
    "            data_start_index = 0\n",
    "        try:\n",
    "            data_stop_index = list(self.__dataframe.index >= _datetime.datetime.strptime(stop, '%d.%m.%Y %H:%M:%S.%f')).index(True)                        \n",
    "        except Exception as e:\n",
    "            rows, columns = self.__dataframe.shape\n",
    "            data_stop_index = rows\n",
    "        dataset = copy.deepcopy(self)\n",
    "        dataset.__dataframe = self.__dataframe.iloc[data_start_index:data_stop_index]\n",
    "        return dataset\n",
    "    \n",
    "    def extract(self, label):\n",
    "        \"\"\" extract a named range defined in meta.json \"\"\"\n",
    "        try:\n",
    "            # get range by label \n",
    "            r = self.__meta[\"ranges\"][label]       \n",
    "            try:\n",
    "                start = r['start']\n",
    "                data_start_index = list(self.__dataframe.index >= _datetime.datetime.strptime(start, '%d.%m.%Y %H:%M:%S.%f')).index(True)                        \n",
    "            except:\n",
    "                data_start_index = 0\n",
    "            try:\n",
    "                stop = r['stop']\n",
    "                data_stop_index = list(self.__dataframe.index >= _datetime.datetime.strptime(stop, '%d.%m.%Y %H:%M:%S.%f')).index(True)                        \n",
    "            except:\n",
    "                rows, columns = self.__dataframe.shape\n",
    "                data_stop_index = rows\n",
    "            dataset = copy.deepcopy(self)\n",
    "            dataset.__dataframe = self.__dataframe.iloc[data_start_index:data_stop_index]\n",
    "            return dataset\n",
    "        except KeyError:\n",
    "            raise(KeyError)\n",
    "\n",
    "    def normalize(self, debug_plot=True, columns=['temp_0', 'temp_1', 'temp_2', 'temp_3', 'temp_4', 'temp_5', 'temp_6', 'temp_7', 'temp_8', 'angle']): \n",
    "        \"\"\" normalize the given columns on the min max of each column \"\"\"           \n",
    "        temperature_and_angle_norm = self.__dataframe.copy()\n",
    "        for column in columns:            \n",
    "            try:\n",
    "                max_value = self.__dataframe[column].max(numeric_only=True)\n",
    "                min_value = self.__dataframe[column].min(numeric_only=True)\n",
    "                temperature_and_angle_norm[column] = (self.__dataframe[column] - min_value) / (max_value-min_value)                        \n",
    "            except:\n",
    "                pass                                \n",
    "        dataset = copy.deepcopy(self)\n",
    "        dataset.__dataframe = temperature_and_angle_norm\n",
    "        return dataset\n",
    "\n",
    "    def minmax(self, columns=['temp_0', 'temp_1', 'temp_2', 'temp_3', 'temp_4', 'temp_5', 'temp_6', 'temp_7', 'temp_8']):\n",
    "        \"\"\" returns the min, max over all given columns \"\"\"\n",
    "        try:\n",
    "            from_min = self.__dataframe[columns].min(numeric_only=True).min()\n",
    "            from_max = self.__dataframe[columns].max(numeric_only=True).max()\n",
    "        except Exception as e:\n",
    "            raise (Exception(\"Cannot detect min,max over all columns for scaling:{}\".format(e)))\n",
    "        return (from_min, from_max)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_multiple_datasets(datasets=[], scale_to=(-1, 1), columns=['temp_0', 'temp_1', 'temp_2', 'temp_3', 'temp_4', 'temp_5', 'temp_6', 'temp_7', 'temp_8']):\n",
    "        \"\"\" Normalize multiple datasets to with a common factor \"\"\"\n",
    "        all_mins = []\n",
    "        all_maxs = []\n",
    "        for dataset in datasets:\n",
    "            mi,ma = dataset.minmax(columns=columns)\n",
    "            all_mins.append(mi)\n",
    "            all_maxs.append(ma)\n",
    "        scale_from = (min(all_mins), max(all_maxs))\n",
    "        for dataset in datasets:\n",
    "            dataset.scale(scale_from=scale_from, scale_to=scale_to, columns=columns)\n",
    "        \n",
    "    def scale(self, scale_from=None, scale_to=(-1, 1), columns=['temp_0', 'temp_1', 'temp_2', 'temp_3', 'temp_4', 'temp_5', 'temp_6', 'temp_7', 'temp_8']):\n",
    "        \"\"\" scale values in columns from scale_from range to scale_to range \n",
    "            if scale_from=None: automatic scale to the min/max values over all columns \n",
    "            \n",
    "            The difference between the normalize and scaling function is that \n",
    "                - normalize scales to the min,max of a single column\n",
    "                - scale scales after looking at all columns (in the column=[] list) or a fixed value if provided\n",
    "            \"\"\"\n",
    "        try:\n",
    "            (to_lower, to_upper) = scale_to\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to unpack scale_to limits: {}\".format(e))\n",
    "        if scale_from is None:\n",
    "            # find the min, max over all columns\n",
    "            try:\n",
    "                from_min = self.__dataframe[columns].min(numeric_only=False).min()\n",
    "                from_max = self.__dataframe[columns].max(numeric_only=False).max()\n",
    "            except Exception as e:\n",
    "                raise (Exception(\"Cannot detect min,max over all columns for scaling:{}\".format(e)))\n",
    "        else:\n",
    "            try:\n",
    "                (from_min, from_max) = scale_from\n",
    "            except Exception as e:\n",
    "                raise Exception(\"Failed to unpack scale_from limits: {}\".format(e))\n",
    "        if from_min > from_max:\n",
    "            raise(Exception(\"scale_from: lower limit larger then upper limit\"))\n",
    "        if to_lower > to_upper:\n",
    "            raise(Exception(\"scale_to: lower limit larger then upper limit\"))\n",
    "        # print((from_min, from_max))\n",
    "        dataset = copy.deepcopy(self)\n",
    "        copyframe = self.__dataframe.copy()        \n",
    "        a=to_lower\n",
    "        b=(to_upper-to_lower)\n",
    "        c=from_min\n",
    "        d=(from_max - from_min)\n",
    "        applied_scale = (a-(b*c)/d, b/d)\n",
    "        for column in columns:\n",
    "            try:\n",
    "                # dataset.__dataframe[column] = dataset.__dataframe[column].apply(lambda x: to_lower + (to_upper-to_lower) * ((x - from_min) / (from_max - from_min)))\n",
    "                copyframe[column] = dataset.__dataframe[column].apply(lambda x: applied_scale[0] + x * applied_scale[1])\n",
    "            except Exception as e:\n",
    "                raise(Exception(\"Cannot scale column '{}': {}\".format(column, e)))\n",
    "        dataset.__dataframe = copyframe\n",
    "        dataset._applied_scale = (dataset._applied_scale[0]+applied_scale[0], dataset._applied_scale[1]*applied_scale[1])\n",
    "        return dataset\n",
    "\n",
    "    def rebase_time(self, date=None, now=True):\n",
    "        \"\"\" change start time of dataset \"\"\"\n",
    "        copyframe = self.__dataframe.copy()        \n",
    "        if now:      \n",
    "            date = _datetime.datetime.today()              \n",
    "        if date is None:\n",
    "            date = _datetime.datetime(1970,0,0,0,0,0)                \n",
    "        offset = date-copyframe.index[0]   \n",
    "        copyframe.index = copyframe.index + offset\n",
    "        dataset = copy.deepcopy(self)\n",
    "        dataset.__dataframe = copyframe    \n",
    "        dataset.meta['rebase_offset'] = offset\n",
    "        return dataset\n",
    "\n",
    "    def get_nearest_index(self, closest_to_time):\n",
    "        \"\"\"get the index closest to a given timestamp \"\"\"\n",
    "        return self.__dataframe.index[self.__dataframe.index.get_loc(closest_to_time, method='nearest')]\n",
    "\n",
    "    def get_named_range(self, name, with_comment=False):\n",
    "        \"\"\" get the meta data of a named range \"\"\"\n",
    "        ra = self.__meta['ranges'][name]\n",
    "        if with_comment:\n",
    "            return ra['start'], ra['stop'], ra['comment']\n",
    "        return ra['start'], ra['stop']\n",
    "\n",
    "    def __str__(self):  \n",
    "        \"\"\" explain dataset \"\"\"\n",
    "        description = []\n",
    "        try:\n",
    "            description.append(\"Dataset: {}\".format(self.__meta['datasets_folder']))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if len(self.__meta['comment']) > 0:\n",
    "                description.append(\"Comment: {}\".format(self.__meta['comment']))\n",
    "        except:\n",
    "            pass\n",
    "        description.append(\"Loaded parts:\")\n",
    "        description.append(\" - Loaded telemetry data: {}\".format(self.contains_telemetry_data))\n",
    "        description.append(\" - Loaded room temperature data: {}\".format(self.contains_room_temperature_data))\n",
    "        description.append(\" - Loaded table data: {}\".format(self.contains_table_data))\n",
    "        description.append(\"Ranges:\")\n",
    "        for r, rr in self.ranges.items():\n",
    "            try:\n",
    "                comment = ''\n",
    "                try: \n",
    "                    if len(rr[\"comment\"]) > 0:\n",
    "                        comment = \": {}\".format(rr[\"comment\"])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                description.append(\" - range '{}': start={}, stop:={}{}\".format(r, rr['start'], rr['stop'], comment))\n",
    "            except:\n",
    "                pass\n",
    "        description.append(\"Anomalies:\")\n",
    "        for rr in self.anomalies:\n",
    "            try:\n",
    "                comment = ''\n",
    "                try: \n",
    "                    if len(rr[\"comment\"]) > 0:\n",
    "                        comment = \": {}\".format(rr[\"comment\"])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                description.append(\" - anomaly of {}% at '{}' during {}s{}\".format(rr['power'], rr['start'], rr['duration'], comment))\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            rows, columns = self.__dataframe.shape\n",
    "            description.append(\"Dataframe has {} data points.\".format(rows))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            description.append(\"Dataset index resolution is: {}\".format(self.__dataframe.index.resolution))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for r in dataset.ranges.keys():\n",
    "            description.append(\"Testing range '{}'\".format(r))\n",
    "            description.append(dataset[r].test())\n",
    "\n",
    "        try:\n",
    "            description.append(\"Dataframe columns:\\r\\n\"+', '.join([\"{}\".format(column) for column in self.__dataframe.columns]))\n",
    "        except:\n",
    "            pass\n",
    "        return '\\r\\n'.join(description) \n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if type(k) == str:\n",
    "            return self.extract(k)\n",
    "        if type(k) == slice:\n",
    "            try:\n",
    "                return self.extract_by_date(k.start, k.stop)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        dataset = copy.deepcopy(self)\n",
    "        dataset.__dataframe = self.__dataframe.iloc[k]\n",
    "        return dataset               \n",
    "\n",
    "    def __getslice__(self, start, end):\n",
    "        if ((type(start) == str) or (type(start) == None)) and ((type(end)==str) or (type(end)==None)):\n",
    "            return self.extract_by_date(start, end)\n",
    "        else:\n",
    "            dataset = copy.deepcopy(self)\n",
    "            dataset.__dataframe = self.__dataframe.iloc[max(0, start):max(0, end)]\n",
    "            return dataset                       \n",
    "\n",
    "    def __load_metadata(self, dataset_name:str, meta_file='meta.json', datasets_folder=['.', 'datasets']):\n",
    "        \"\"\" Load dataset meta data from meta.json \"\"\"     \n",
    "        try:\n",
    "            datasets_path = os.path.join(*datasets_folder)\n",
    "            with open(os.path.join(datasets_path, dataset_name, meta_file)) as metafile:\n",
    "                self.__meta = json.load(metafile)\n",
    "                self.__meta['datasets_folder'] = os.path.join(datasets_path, dataset_name)       \n",
    "        except FileNotFoundError:\n",
    "            raise(Exception(\"Cannot find meta file for dataset {}\".format(dataset_name)))\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise(Exception(\"Error in meta.json: {}\".format(e)))\n",
    "        self.__parse_anomalies()\n",
    "\n",
    "    def __parse_anomalies(self):\n",
    "        offset = self.__anomalies_time_offset(self.__meta[\"anomalies\"])\n",
    "        for anomaly in self.__meta[\"anomalies\"][\"data\"]:\n",
    "            try:\n",
    "                time = _datetime.datetime.strptime(anomaly['start'], '%d.%m.%Y %H:%M:%S.%f')\n",
    "                anomaly['start'] = time + offset \n",
    "            except:\n",
    "                print(\"Failed to parse anomaly date: {}\".format(anomaly))\n",
    "\n",
    "    def __anomalies_time_offset(self, context:dict, label:str=\"anomalies\"):\n",
    "        \"\"\" find time offset correction for anomalies\n",
    "            offset: {\"days\":0, \"seconds\":0, \"microseconds\":0, \"milliseconds\":0, \"minutes\":0, \"hours\":0, \"weeks\":0}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tc_map = context[\"time_correction\"]\n",
    "            try:\n",
    "                if tc_map['offset'] is None:\n",
    "                    raise TypeError\n",
    "                delta_datetime = _datetime.timedelta(**tc_map['offset'])\n",
    "                if self.verbose:\n",
    "                    print(\"Corrected time for '{}' using given offset. {}\".format(label, delta_datetime))\n",
    "            except Exception:  #TypeError:\n",
    "                try:\n",
    "                    recorded_datetime = _datetime.datetime.strptime(tc_map['recorded_datetime'], '%d.%m.%Y %H:%M:%S.%f')\n",
    "                except KeyError:\n",
    "                    recorded_datetime = _datetime.datetime.fromtimestamp(float(tc_map['recorded_datetime']))\n",
    "                try:\n",
    "                    dataframe_datetime = _datetime.datetime.strptime(tc_map['dataframe_datetime'], '%d.%m.%Y %H:%M:%S.%f')\n",
    "                except KeyError:\n",
    "                    dataframe_datetime = _datetime.datetime.fromtimestamp(float(tc_map['dataframe_datetime']))\n",
    "                delta_datetime =  dataframe_datetime - recorded_datetime\n",
    "                if self.verbose:\n",
    "                    print(\"Corrected {} time using given datetimes.\".format(label))\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Time offset corrected for '{}' by {}.\".format(label.capitalize(), delta_datetime))\n",
    "\n",
    "        except Exception as e:\n",
    "            delta_datetime = _datetime.timedelta({'hours':0})\n",
    "            if self.verbose:\n",
    "                print(\"No time correction has been provided for {}. Assuming clocks to the synchronous.({})\".format(label, e))\n",
    "        return delta_datetime \n",
    "\n",
    "           \n",
    "    def __load(self, dataset_name:str, sin_of_angle=True, datasets_folder=['.', 'datasets'], meta_file='meta.json'):\n",
    "        \"\"\" load the dataset \"\"\"\n",
    "        self.__load_metadata(dataset_name=dataset_name, meta_file=meta_file, datasets_folder=datasets_folder)\n",
    "        self.__load_telemetry_data()\n",
    "        self.__load_table_data()\n",
    "        self.__load_room_temperature_data()\n",
    "        self.__loaded_dataframe = self.__dataframe\n",
    "\n",
    "    def __merge_partial_dataframe(self, dataframe):\n",
    "        \"\"\" merge new dataframe into this dataset \"\"\"\n",
    "        if not self.__dataframe is None:\n",
    "            if self.verbose:\n",
    "                print(\"Source resolution:{}, target resolution: {}\".format(dataframe.index.resolution, self.__dataframe.index.resolution))\n",
    "            # reindex on existing data\n",
    "            reindexed_dataframe=(dataframe\n",
    "                 # .reindex(self.__dataframe.index.union(self.__dataframe.index))\n",
    "                .reindex(self.__dataframe.index.union(dataframe.index))\n",
    "                .interpolate(method='time', limit_direction='both')                \n",
    "                #.reindex(self.__dataframe.index)\n",
    "            )\n",
    "            self.__dataframe = self.__dataframe.join(reindexed_dataframe)\n",
    "        else:\n",
    "            self.__dataframe = dataframe\n",
    "\n",
    "\n",
    "    def __time_correction(self, dataframe, context:dict, label:str):\n",
    "        \"\"\" time offset correction \n",
    "            offset: {\"days\":0, \"seconds\":0, \"microseconds\":0, \"milliseconds\":0, \"minutes\":0, \"hours\":0, \"weeks\":0}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tc_map = context[\"time_correction\"]\n",
    "            try:\n",
    "                if tc_map['offset'] is None:\n",
    "                    raise TypeError\n",
    "                delta_datetime = _datetime.timedelta(**tc_map['offset'])\n",
    "                if self.verbose:\n",
    "                    print(\"Corrected time for '{}' using given offset. {}\".format(label, delta_datetime))\n",
    "            except Exception:  #TypeError:\n",
    "                try:\n",
    "                    recorded_datetime = _datetime.datetime.strptime(tc_map['recorded_datetime'], '%d.%m.%Y %H:%M:%S.%f')\n",
    "                except KeyError:\n",
    "                    recorded_datetime = _datetime.datetime.fromtimestamp(float(tc_map['recorded_datetime']))\n",
    "                try:\n",
    "                    dataframe_datetime = _datetime.datetime.strptime(tc_map['dataframe_datetime'], '%d.%m.%Y %H:%M:%S.%f')\n",
    "                except KeyError:\n",
    "                    dataframe_datetime = _datetime.datetime.fromtimestamp(float(tc_map['dataframe_datetime']))\n",
    "                delta_datetime =  dataframe_datetime - recorded_datetime\n",
    "                if self.verbose:\n",
    "                    print(\"Corrected time for '{}' using given datetimes.\".format(label))\n",
    "\n",
    "            # offset correct dataframe index\n",
    "            index_as_list = dataframe.index\n",
    "            index_as_list += delta_datetime \n",
    "            dataframe.index = index_as_list\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"Time offset corrected for '{}' by {}.\".format(label.capitalize(), delta_datetime))\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(\"No time correction has been provided for {}. Assuming clocks to the synchronous.({})\".format(label, e))\n",
    "        return dataframe\n",
    "\n",
    "    def __get_csv_parameters(self, context, delimiter=';', decimal=',', encoding=\"unicode_escape\"):\n",
    "        \"\"\" returns csv information from the meta data or if not provided defaults \"\"\"\n",
    "        try:\n",
    "            delimiter = context[\"delimiter\"]\n",
    "        except:\n",
    "            delimiter = \";\"\n",
    "        try:\n",
    "            decimal = context[\"decimal\"]\n",
    "        except:\n",
    "            decimal = \",\"\n",
    "        try:\n",
    "            encoding = context[\"encoding\"]\n",
    "        except:\n",
    "            encoding = \"unicode_escape\"\n",
    "        return (delimiter, decimal, encoding)\n",
    "\n",
    "    def __load_telemetry_data(self, label=\"telemetry data\"):\n",
    "        \"\"\" Load telemetry data \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Loading {}\".format(label))\n",
    "        try:\n",
    "            context = self.__meta['sources']['eps_data']\n",
    "\n",
    "            # do not load if this block is disabled\n",
    "            try:\n",
    "                if context['enabled'] == False:\n",
    "                    return\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # find telemetry data \n",
    "            satellite_logs = os.path.join(self.__meta['datasets_folder'], *context['directory'])\n",
    "            if not os.path.exists(satellite_logs):                        \n",
    "                raise(Exception(\"Dataset '{}' does not exist in {}\".format(self.dataset_name, satellite_logs)))\n",
    "            telemetry_files = [os.path.join(x[0], context['filename']) for x in os.walk(satellite_logs) if os.path.exists(os.path.join(x[0], context['filename']))]\n",
    "            telemetry_files.sort()\n",
    "            if self.verbose:\n",
    "                print(\"Detected telemetry files:\")\n",
    "                print(telemetry_files)\n",
    "            if len(telemetry_files) == 0:            \n",
    "                raise(Exception(\"Dataset is empty or path not found! ({})\".format(self.dataset_name)))\n",
    "\n",
    "            __dateparser = lambda x: pd.to_datetime(x, unit='s', origin='unix', utc=False)# .round('S')\n",
    "\n",
    "            (delimiter, decimal, encoding) = self.__get_csv_parameters(context, delimiter=\",\", decimal=\".\", encoding=\"unicode_escape\")\n",
    "            dataframes={}            \n",
    "            for telemetry_file in telemetry_files:\n",
    "                try:\n",
    "                    dataframes[telemetry_file] = pd.read_csv(telemetry_file,  delimiter=delimiter, decimal=decimal, encoding=encoding, index_col='timestamp', parse_dates=['timestamp'], date_parser=__dateparser)\n",
    "                except Exception as e:\n",
    "                    if self.verbose:\n",
    "                        print(\"Failed to load telemetry file {}:{}\".format(telemetry_file, e))\n",
    "\n",
    "            # merge data into a single frame \n",
    "            dataframe = pd.concat(dataframes)\n",
    "            dataframe = dataframe.droplevel(level=0)\n",
    "            dataframe = dataframe.sort_index()\n",
    "            dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "            # apply time correction \n",
    "            dataframe = self.__time_correction(dataframe, context, label)\n",
    "\n",
    "            # store dataframe unmerged\n",
    "            self.__partial_dataframes['telemetry']=dataframe\n",
    "\n",
    "            # merge dataframe\n",
    "            self.__merge_partial_dataframe(dataframe)\n",
    "\n",
    "            # mark as loaded\n",
    "            self.contains_telemetry_data = True\n",
    "        except FileNotFoundError:\n",
    "            dataframe = None\n",
    "            self.contains_telemetry_data = False\n",
    "\n",
    "    def __load_table_data(self, label=\"table data\"):\n",
    "        \"\"\" Load table data \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Loading {}\".format(label))\n",
    "        try:\n",
    "            context = self.__meta['sources']['table_data']\n",
    "            \n",
    "            # do not load if this block is disabled\n",
    "            try:\n",
    "                if context['enabled'] == False:\n",
    "                    return\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            (delimiter, decimal, encoding) = self.__get_csv_parameters(context, delimiter=\"; \", decimal=\",\", encoding=\"unicode_escape\")\n",
    "            __dateparser = lambda x: pd.to_datetime(x, unit='s', origin='unix', utc=False)# .round('S')\n",
    "            angle_file = os.path.join(self.__meta['datasets_folder'], *context['directory'], context['filename'])\n",
    "            dataframe = pd.read_csv(angle_file, delimiter=delimiter, decimal=decimal, encoding=encoding, index_col='timestamp', parse_dates=['timestamp'], date_parser=__dateparser, engine='python')\n",
    "           \n",
    "            # add start angle\n",
    "            try:\n",
    "                # print(dataframe['angle'])\n",
    "                dataframe['angle'] = dataframe['angle'] + float(context['start_angle'])\n",
    "                if self.verbose:\n",
    "                    print(\"Corrected angle by {} degrees for {}\".format( float(context['start_angle']), label))\n",
    "                # print(dataframe['angle'])\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(\"No start angle provided for {}: {}\".format(label, e))\n",
    "\n",
    "            # remove unnecessary column\n",
    "            try:\n",
    "                del dataframe['target']\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            try:\n",
    "                del dataframe['t_delta']\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                if context['wrap_angle']:\n",
    "                    dataframe['angle'] = np.mod(dataframe['angle'], 360.0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                if context['add_sin_of_angle']:\n",
    "                    angle_sin = np.sin(dataframe['angle']/360*2*math.pi)\n",
    "                    if self.separate_angle_sin:\n",
    "                        dataframe['sin_of_angle'] = angle_sin\n",
    "                    else:\n",
    "                        dataframe['angle'] = angle_sin\n",
    "                    self.contains_sin_of_angle = True\n",
    "            except KeyError:\n",
    "                self.contains_sin_of_angle = False\n",
    "\n",
    "            # apply time correction \n",
    "            dataframe = self.__time_correction(dataframe, context, label)\n",
    "\n",
    "            # store dataframe unmerged\n",
    "            self.__partial_dataframes['table']=dataframe\n",
    "\n",
    "            # merge dataframe\n",
    "            self.__merge_partial_dataframe(dataframe)\n",
    "           \n",
    "            # mark as loaded\n",
    "            self.contains_table_data = True\n",
    "        except FileNotFoundError:\n",
    "            if self.verbose:\n",
    "                print(e)\n",
    "            self.contains_table_data = False\n",
    "\n",
    "    def __load_room_temperature_data(self, label=\"room temperature data\"):\n",
    "        \"\"\" Load room temperature data \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Loading {}\".format(label))\n",
    "        try:\n",
    "            context = self.__meta['sources']['room_temperature_data']\n",
    "\n",
    "             # do not load if this block is disabled\n",
    "            try:\n",
    "                if context['enabled'] == False:\n",
    "                    return\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            room_temperature_file = os.path.join(self.__meta['datasets_folder'], *context['directory'], context['filename'])\n",
    "            (delimiter, decimal, encoding) = self.__get_csv_parameters(context, delimiter=\";\", decimal=\",\", encoding=\"unicode_escape\")\n",
    "            __dateparser = lambda x: _datetime.datetime.strptime(x, '%d.%m.%y %H:%M:%S,%f')\n",
    "            dataframe = pd.read_csv(room_temperature_file, index_col=3, parse_dates=['Start Time'], date_parser=__dateparser, delimiter=delimiter, decimal=decimal, encoding=encoding)\n",
    "            dataframe = dataframe[[\"Average\"]]\n",
    "            dataframe.index.names = [\"Timestamp\"]\n",
    "            dataframe = dataframe.rename(columns={\"Average\":\"room_temperature\"})\n",
    "\n",
    "            # apply time correction \n",
    "            dataframe = self.__time_correction(dataframe, context, label)\n",
    "\n",
    "            # store dataframe unmerged\n",
    "            self.__partial_dataframes['room_temperature']=dataframe\n",
    "\n",
    "            # merge dataframe\n",
    "            self.__merge_partial_dataframe(dataframe)       \n",
    "            \n",
    "            # mark as loaded\n",
    "            self.contains_room_temperature_data = True\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(e)\n",
    "            self.contains_room_temperature_data = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'enable_example' in locals():\n",
    "    enable_example=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset and plot it with anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset = Dataset('2022.06.15', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect sampling period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    period, deviation = dataset[\"experiment\"].period\n",
    "    print(\"Detected sampling period: {}[s] with a standard deviation of {}[s]\".format(period, deviation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the merged dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset.save_dataframe_as_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting (with anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset.plot().plot_ranges().plot_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check linearity of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset.plot_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Room temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    try:\n",
    "        dataset[\"experiment\"].plot(column='room_temperature')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot table angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset['experiment'].normalize().plot(columns=['sin_of_angle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset[\"reboot\"].normalize().plot().plot_ranges().plot_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset[\"experiment\"].plot(more_columns=['room_temperature'], not_columns=['angle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    try:\n",
    "        print(dataset[\"experiment\"].dataframe.shape)\n",
    "        dataset[\"experiment\"].plot(column='room_temperature')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    try:\n",
    "        print(dataset[\"experiment\"]['normal'].dataframe.shape)\n",
    "        dataset[\"experiment\"].extract('normal').plot(column='room_temperature')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data since last satellite reboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    dataset[\"reboot\"].plot(not_columns=['angle'], more_columns=['room_temperature']).plot_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset extracting and slicing\n",
    "Extract the experiment part (between start and stop), normalize the data and plot. Operations can be chained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    try:\n",
    "        experiment = dataset['experiment'].normalize().plot().plot_anomalies()\n",
    "        # or \n",
    "        experiment = dataset.extract('experiment').normalize().plot().plot_anomalies()\n",
    "    except KeyError:\n",
    "        print(\"Range does not exist on dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    try:\n",
    "        dataset['15.06.2022 10:30:00.00':'15.06.2022 17:00:00.00'].normalize().plot()\n",
    "        # or \n",
    "        dataset.extract_by_date('15.06.2022 10:30:00.00','15.06.2022 17:00:00.00').normalize().plot()\n",
    "    except KeyError:\n",
    "        print(\"Not data for this date range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abnormal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    try:\n",
    "        experiment = dataset['abnormal'].normalize().plot().plot_anomalies()\n",
    "    except KeyError:\n",
    "        print(\"No abnormal data defined for this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing dataset returns a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    experiment[0:100].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing pandas dataframe from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    experiment.dataframe['temp_0'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebase time \n",
    "(useful to overlay multiple datasets from different days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    experiment.rebase_time().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the dataset\n",
    " 1. fix scaling from -10,10 to -1,1\n",
    " 2. scaling from (min, max) over all columns to -1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    print(dataset['normal'].scale(scale_from=(-10, 10), scale_to=(-1, 1)).plot(columns=['temp_0', 'temp_1', 'temp_2']).scale_factor)\n",
    "    print(dataset['normal'].scale(scale_from=None, scale_to=(-1, 1)).plot(columns=['temp_0', 'temp_1', 'temp_2']).scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test reseting the dataset\n",
    "A range is getting selected and a scale applied to the dataset. Then it is getting reset. The result should revert the operations done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:    \n",
    "    print(dataset['normal'].scale(scale_from=None, scale_to=(-1, 1)).reset().plot(columns=['temp_0', 'temp_1', 'temp_2']).scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests and validations - Checking dataset validity\n",
    "Run test on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    print(\"Dataset '{}' {} over the selected range.\".format(experiment.name, \"is valid\" if experiment.is_valid else \"is invalid.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain all tests done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    print(experiment.test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests for all defined ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    print(dataset.test_all_ranges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your own tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    class DatasetTest_SamplingPeriod(DatasetTest):\n",
    "        def __init__(self, period=5.0, epsilon=0.2):\n",
    "            self.period = period\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "        @property\n",
    "        def name(self):\n",
    "            return \"sampling period close to target period\"\n",
    "\n",
    "        def test(self, dataset):\n",
    "            p,s = dataset.period\n",
    "            return abs(p-self.period) < self.epsilon\n",
    "\n",
    "    print(experiment.test(tests=[DatasetTest_SamplingPeriod(period=5.0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display angle at anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_example:\n",
    "    for anomaly in experiment.anomalies:\n",
    "        anomaly_time = anomaly['start']\n",
    "        idx=experiment.get_nearest_index(anomaly_time)\n",
    "        angle = experiment.dataframe['angle'][idx]\n",
    "        print(\"@{} angle is {}\".format(anomaly_time, angle))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
