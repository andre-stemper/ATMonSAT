{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copyright 2022 University of Luxembourg\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "> you may not use this file except in compliance with the License.  \n",
    "> You may obtain a copy of the License at  \n",
    ">\n",
    ">    https://www.apache.org/licenses/LICENSE-2.0\n",
    ">\n",
    "> Unless required by applicable law or agreed to in writing, software  \n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "> See the License for the specific language governing permissions and  \n",
    "> limitations under the License.  \n",
    ">\n",
    "***\n",
    "\n",
    "Author: André Stemper (andre.stemper@uni.lu)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AtMonSat Detection Algorithm Test Bench\n",
    "\n",
    "Datapoints from a dataset are send to the C++ implementation for evaluation and results are recorded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "Microcontroller / processor frequency (this value is used to select some settings (e.g. baudrate) and define the filename to store the results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microcontroller_frequency = '298' # [MHz]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connection type**  \n",
    "A connection can be done to an implementation compiled for the host computer using pipes or to an implementation running on a microcontroller (e.g. STM32H743) using serial communication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_type = 'local'\n",
    "#connection_type = 'remote'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings for a remote connection**  \n",
    "connection_baudrate is the baudrate to use for the communication with the microntroller. This setting must match the baurate defined in the firmware.  \n",
    "connection_port is the interface on the local machine that connects to the remote microcontroller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_baudrate = {'298': 3686400,\n",
    "                       '146': 1843200,\n",
    "                       '78': 921600,\n",
    "                       '39': 460800}[microcontroller_frequency]  # this must match the baudrate of the firmware\n",
    "connection_port = '/dev/ttyACM0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings for a local connection**  \n",
    "connection_local_command is a list with the command as the first member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_local_command = ['../firmwares/atmonsat_native/atmonsat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset to transmit**   \n",
    "All datasets are located in the datasets directory. The use_dataset variable defines which dataset to use.\n",
    "\n",
    "These measurements have been taken from the EPS of a cubesat. TEMP_0 to TEMP_3 are MPPT temperatures TEMP_4 to TEMP_8 are voltage regulator temperatures and TEMP_9 represents the battery temperature.  \n",
    "All temperatures have a resolution of 1°C. \n",
    "\n",
    "Several ranges have been defined for each dataset. \n",
    " - 'normal': range where no anomaly happens.\n",
    " - 'abnormal': range where one or multiple anomalies have been introduced.\n",
    " - 'experiment': normal + abnormal range.\n",
    "  \n",
    "The use_range variable can be used to select what range of datapoints to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dataset = '2022.07.20'\n",
    "use_range = 'experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of datapoints**  \n",
    "limit_number_of_datapoints limits the number of datapoints from dataset to be transmitted for evaluation.\n",
    " - None: all datapoints will be transmitted\n",
    " - any integer > 0: Maximum number of datapoints to transmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_number_of_datapoints = None # <int> or None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threshold hold-off**  \n",
    "A hold-off can be activated that suppresses any further detection after an anomaly detection during the number of specified iterations.  \n",
    "threshold_hold_off can take any positive integer value. 0 disables the postprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_hold_off = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output directory (where to store the results). A directory will be created inside this output directory for each measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "timestamp_string = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_directory = output_directory + os.path.sep + timestamp_string\n",
    "os.mkdir(output_directory)\n",
    "print(\"Results store in {}\".format(output_directory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save experiment settings for later reference into meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "settings = {\n",
    "    'timestamp': timestamp_string,\n",
    "    'dataset': use_dataset,\n",
    "    'range': use_range,\n",
    "    'frequency': microcontroller_frequency,\n",
    "    'connection_type': connection_type,\n",
    "    'limit': limit_number_of_datapoints,\n",
    "    'threshold_hold_off': threshold_hold_off,\n",
    "}\n",
    "\n",
    "print('Settings:')\n",
    "for k, v in settings.items():\n",
    "    print(\" - {} = {}\".format(k, v))\n",
    "\n",
    "filename = output_directory + os.path.sep + 'settings.json'\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(settings, file)\n",
    "    print(\"Settings saved to '{}'\".format(filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "enable_example = False\n",
    "%run dataset.ipynb\n",
    "%run protocol.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n",
    "Information, warning and errors are getting logged into atmonsat_testbench.txt.\n",
    "The variable display_log is a boolean that defines if the same information should be visualized on screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_log = True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='atmonsat_testbench.txt',\n",
    "                    format='%(levelname)s:%(message)s',\n",
    "                    encoding='utf-8',\n",
    "                    filemode='w',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "if display_log:\n",
    "    logging.getLogger().addHandler(logging.StreamHandler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is getting called by the communication protocol and can be used to visualize data exchange between the host and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(s):\n",
    "    # print(s)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "**Load the specified dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(use_dataset)[use_range]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**  \n",
    "Features to be packed into a datapoint and send to the implementaion. The order of the features must correspond to the ordering of a datapoint in the C++ implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['temp_0', 'temp_1', 'temp_2', 'temp_3',\n",
    "                   'temp_4', 'temp_5', 'temp_6', 'temp_7', 'temp_8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the selectted range of the dataset for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(8)\n",
    "dataset.plot(not_columns=['angle', 'sin_of_angle'],\n",
    "             ax=ax).plot_anomalies(ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datapoints**  \n",
    "Prepare a list of datapoint commands to be send to the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = []\n",
    "for index, row in dataset.dataframe[feature_columns].iterrows():\n",
    "    if not limit_number_of_datapoints is None:\n",
    "        if len(commands) > limit_number_of_datapoints:\n",
    "            break\n",
    "    commands.append(CommandDatapointINT8(data=row.to_numpy(dtype=np.int8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datastore**  \n",
    "Datastore is a class that collects incoming distance and detection responses from the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStore(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__df = pd.DataFrame(\n",
    "            columns=['detection_recording_timestamp', 'distance', 'detection'])\n",
    "        self.__row = 0\n",
    "        # distance and detection arrive independent of each other: wait for both, then store the row\n",
    "        self.__distance_arrived = False\n",
    "        self.__detection_arrived = False\n",
    "        self.__recorded_distance = 0\n",
    "        self.__recorded_detection = False\n",
    "\n",
    "    @property\n",
    "    def dataframe(self):\n",
    "        return self.__df\n",
    "\n",
    "    def save(self, filename=\"distance_detection.csv\", sep=\";\"):\n",
    "        self.__df.to_csv(filename, sep=sep)\n",
    "\n",
    "    def __store(self):\n",
    "        if self.__distance_arrived and self.__detection_arrived:\n",
    "            # Store received row\n",
    "            self.__df.loc[self.__row] = [time.time(),\n",
    "                                         self.__recorded_distance, self.__recorded_detection]\n",
    "            self.__row = self.__row + 1\n",
    "            self.__distance_arrived = False  # reset flag\n",
    "            self.__detection_arrived = False  # reset flag\n",
    "\n",
    "    def store_distance(self, distance):\n",
    "        # logging.info(\"Mahalanobis distance={}\".format(distance))\n",
    "        self.__recorded_distance = distance\n",
    "        if self.__distance_arrived:\n",
    "            logging.error(\n",
    "                \"Received distance a second time before detection. This should not happen.\")\n",
    "        self.__distance_arrived = True\n",
    "        self.__store()\n",
    "\n",
    "    def store_detection(self, detection):\n",
    "        # logging.info(\"Detection={}\".format(detection))\n",
    "        self.__recorded_detection = detection\n",
    "        if self.__detection_arrived:\n",
    "            logging.error(\n",
    "                \"Received detection a second time before distance. This should not happen.\")\n",
    "        self.__detection_arrived = True\n",
    "        self.__store()\n",
    "\n",
    "\n",
    "datastore = DataStore()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ExecutionTimeStore**  \n",
    "ExecutionTimeStore is a class that collects incoming execution_time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutionTimeStore(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__df = pd.DataFrame(columns=['execution_time_recording_timestamp', 'execution_time'])\n",
    "        self.__row = 0\n",
    "\n",
    "    @property\n",
    "    def dataframe(self):\n",
    "        return self.__df\n",
    "\n",
    "    def response_handler_callback_execution_time(self, duration):\n",
    "        # logging.info(\"Execution time={}\".format(duration))\n",
    "        self.__df.loc[self.__row] = [time.time(), float(duration*1e-9)]\n",
    "        self.__row = self.__row + 1\n",
    "\n",
    "    def save(self, filename=\"execution_times.csv\", sep=\";\"):\n",
    "        self.__df.to_csv(filename, sep=sep)\n",
    "\n",
    "execution_time_store = ExecutionTimeStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response_handler_callback_remote_exception is a response handler that is getting called when an exception occured in the implementation (local or remote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_handler_callback_remote_exception(str):\n",
    "    text = \"A remote exception occurred: {}\".format(str)\n",
    "    logging.fatal(\"Exception={}\".format(text))\n",
    "    raise Exception(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "**Communication response handlers**   \n",
    "response_handlers is a list that defines all allowed responses from the microcontroller and what handlers to call after reception.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_handlers = [\n",
    "    CommandInt32(PROTOCOL_COMMAND_ID_INT32,\n",
    "                 callback=lambda d: logging.info(\"Got int32={}\".format(d))),\n",
    "    CommandFloat(PROTOCOL_COMMAND_ID_FLOAT,\n",
    "                 callback=lambda d: logging.info(\"Got float={}\".format(d))),\n",
    "    CommandFloat(PROTOCOL_COMMAND_ID_MAHALANOBIS_DISTANCE,\n",
    "                 callback=datastore.store_distance),\n",
    "    CommandUInt8(PROTOCOL_COMMAND_ID_DETECTION,\n",
    "                 callback=datastore.store_detection),\n",
    "    CommandComment(PROTOCOL_COMMAND_ID_COMMENT,\n",
    "                   callback=lambda d: logging.info(\"Comment={}\".format(d))),\n",
    "    CommandComment(PROTOCOL_COMMAND_ID_EXCEPTION, callback=response_handler_callback_remote_exception ),\n",
    "    CommandUInt32(PROTOCOL_COMMAND_ID_EXECUTION_TIME, callback=execution_time_store.response_handler_callback_execution_time)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to the target**  \n",
    "Connection can be done on a local target (compiled for the host) or a remote target over serial communication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if connection_type == \"local\":\n",
    "    connection = ConnectionSubprocess(command=connection_local_command,\n",
    "                                      response_handlers=response_handlers)\n",
    "elif connection_type == \"remote\":\n",
    "    connection = ConnectionSerial(port=connection_port,\n",
    "                                  baudrate=connection_baudrate,\n",
    "                                  response_handlers=response_handlers)\n",
    "else:\n",
    "    print(\"Connection type must be 'pipe' or 'serial' \")\n",
    "\n",
    "connection.open()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize target**  \n",
    "This command initializes or resets the implementation.\n",
    "It has been implemented as a command as the initialization code might try to report errors which is a communication from target->host that is only allowed in reply to a command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.emit(CommandInitialize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload coefficients**  \n",
    "Upload coefficients stored in a save_variables.pkl to the implementation. The uploaded coefficients are:\n",
    "- detection_threshold\n",
    "- mahalanobis inverse covariance matrix \n",
    "- mahalanobis mean vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'saved_variables.pkl'\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        variables_dict = pickle.load(file)\n",
    "        # logging.info(\"Threshold: {}\".format(float(variables_dict['detection_threshold'])))\n",
    "        # logging.info(\"Matrix: {}\".format(np.array(variables_dict['mahalanobis_matrix']).flatten()))\n",
    "        # logging.info(\"Mean: {}\".format(np.array(variables_dict['mahalanobis_mean']).flatten()))\n",
    "\n",
    "        connection.emit(CommandFloat(id=PROTOCOL_COMMAND_ID_THRESHOLD,\n",
    "                                     data=float(variables_dict['detection_threshold'])))\n",
    "        connection.emit(CommandMatrix(id=PROTOCOL_COMMAND_ID_MAHALANOBIS_INVERSE_COVARIANCE_MATRIX,\n",
    "                                      data=np.array(variables_dict['mahalanobis_matrix']).flatten()))\n",
    "        connection.emit(CommandMatrix(id=PROTOCOL_COMMAND_ID_MAHALANOBIS_MEAN,\n",
    "                                      data=np.array(variables_dict['mahalanobis_mean']).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection.emit(CommandFloat(id=PROTOCOL_COMMAND_ID_THRESHOLD, data=float(1e6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload hold-off**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.emit(CommandUInt32(id=PROTOCOL_COMMAND_ID_THRESHOLD_HOLD_OFF, data=threshold_hold_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n",
    "Take time before sending all datapoints to estimate the total required time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload the datapoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.emit(commands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total required time**  \n",
    "This is the time of the evaluation + testbench and communication overhead. It gives an uppper bound on the duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_evaluation_time = time.time() - start_time\n",
    "print(\"Duration of evaluation + communication + testbench: {} [s]\".format(total_evaluation_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the received data and extend the dataset with responses**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore.save(filename = output_directory + os.path.sep + \"detection.csv\")\n",
    "execution_time_store.save(filename = output_directory + os.path.sep + \"execution_time.csv\")\n",
    "\n",
    "missing_values = len(dataset.dataframe.index) - len(datastore.dataframe['detection'])\n",
    "print(\"Number of missing detection responses: {} / {}\".format(missing_values, len(dataset.dataframe.index)))\n",
    "dataset.dataframe['detection'] = list(datastore.dataframe['detection']) + [0]*missing_values\n",
    "dataset.dataframe['distance'] = list(datastore.dataframe['distance']) + [0]*missing_values\n",
    "dataset.dataframe['detection_recording_timestamp'] = list(datastore.dataframe['detection_recording_timestamp']) + [0]*missing_values\n",
    "\n",
    "missing_values = len(dataset.dataframe.index) - len(execution_time_store.dataframe['execution_time'])\n",
    "print(\"Number of missing execution_time responses: {} / {}\".format(missing_values, len(dataset.dataframe.index)))\n",
    "dataset.dataframe['execution_time'] = list(execution_time_store.dataframe['execution_time']) + [0]*missing_values\n",
    "dataset.dataframe['execution_time_recording_timestamp'] = list(execution_time_store.dataframe['execution_time_recording_timestamp']) + [0]*missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the dataset with the results to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = output_directory + os.path.sep + \"testbench_data.csv\"\n",
    "print(\"Dataset and results have been stored into: {}\".format(filename))\n",
    "dataset.save_dataframe_as_csv(filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing \n",
    "**Visualize the dataset and the results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ylabel_position = (-0.030, 0.5)\n",
    "fig, axs = plt.subplots(4, 1, sharex=True)\n",
    "fig.set_figwidth(26)\n",
    "fig.set_figheight(15)\n",
    "axs[0].set_title(\"Dataset {} evaluated on {}@{}MHz, ({})\".format(dataset.dataset_name, connection_type, microcontroller_frequency, timestamp_string),fontsize=16)\n",
    "axs[0].set_ylabel(\"[°C]\")\n",
    "axs[0].yaxis.set_label_coords(*ylabel_position)\n",
    "dataset.plot(columns=feature_columns, ax=axs[0]).plot_anomalies(ax=axs[0])\n",
    "axs[1].set_ylabel(\"Mahalanobis distance squared\")\n",
    "axs[1].yaxis.set_label_coords(*ylabel_position)\n",
    "dataset.plot(columns=['distance'], ax=axs[1], grid=True).plot_anomalies(ax=axs[1])\n",
    "axs[1].grid(axis='x')\n",
    "axs[2].set_ylabel(\"Detection\")\n",
    "axs[2].yaxis.set_label_coords(*ylabel_position)\n",
    "axs[2].set_yticks([1.0, 0.0], [\"abnormal\", \"normal\"])\n",
    "dataset.plot(columns=['detection'], ax=axs[2]).plot_anomalies(ax=axs[2])\n",
    "axs[3].set_ylabel(\"Execution time [s]\")\n",
    "axs[3].yaxis.set_label_coords(*ylabel_position)\n",
    "dataset.plot(columns=['execution_time'], ax=axs[3], grid=True).plot_anomalies(ax=axs[3])\n",
    "axs[3].grid(axis='x')\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "filename = output_directory + os.path.sep + \"testbench_results.png\"\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution time statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = dataset.dataframe['execution_time']\n",
    "\n",
    "execution_time_statistics = {\"mean\": float(column.mean()),\n",
    "              \"median\": float(column.median()),\n",
    "              \"std\": float(column.std()),\n",
    "              \"min\": float(column.min()),\n",
    "              \"max\": float(column.max()),\n",
    "              \"sum\": float(column.sum()),\n",
    "              \"sample_size\": len(column)\n",
    "              }\n",
    "\n",
    "print(\"Execution time / datapoint statistics\")\n",
    "for k, v in execution_time_statistics.items():\n",
    "    print(\"{} \\t= {} [s]\".format(k, v))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Description\")\n",
    "print(column.describe())\n",
    "\n",
    "statistics = {\"execution_time\": execution_time_statistics}\n",
    "\n",
    "filename = output_directory + os.path.sep + \"execution_time_statistics.json\"\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(statistics, file)  \n",
    "    print(\"Statistics stored to '{}'\".format(filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histogram of excution duration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if connection_type == \"local\":\n",
    "    ax=column.plot.hist(column=[\"execution_time\"], bins=15, logy=True, xlim=[0,0.001], figsize=(15,8), edgecolor='white', linewidth=3, grid=True)\n",
    "else:\n",
    "    ax=column.plot.hist(column=[\"execution_time\"], bins=15, logy=True, xlim=[0, 0.5], figsize=(15,8), edgecolor='white', linewidth=3, grid=True)\n",
    "\n",
    "ax.set_xlabel(\"Execution time [s]\")\n",
    "ax.set_ylabel(\"Frequency of occurrences [1]\")\n",
    "\n",
    "filename = output_directory + os.path.sep + \"testbench_histogram.png\"\n",
    "plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Communication and testbench overhead**  \n",
    "Calculate time overhead due to communication and testbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_overhead = total_evaluation_time - column.sum()\n",
    "print(\"Total duration: {} [s]\".format(total_evaluation_time))\n",
    "print(\"Evaluation duration: {} [s]\".format(column.sum()))\n",
    "print(\"Overhead (communication and testbench): {} [s]\".format(time_overhead))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize\n",
    "**Terminate the connection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
