{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copyright 2022 University of Luxembourg\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "> you may not use this file except in compliance with the License.  \n",
    "> You may obtain a copy of the License at  \n",
    ">\n",
    ">    https://www.apache.org/licenses/LICENSE-2.0\n",
    ">\n",
    "> Unless required by applicable law or agreed to in writing, software  \n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "> See the License for the specific language governing permissions and  \n",
    "> limitations under the License.  \n",
    ">\n",
    "***\n",
    "\n",
    "Author: Andrzej Mizera (andrzej.mizera@uni.lu)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# AtMonSat Anomaly Detection Algorithm: training, validation, and testing of the deep-learning model\n",
    "\n",
    "This notebook is used to train a deep-learning model for the anmonaly detection algorithm developed within the AtMonSat project. The trained model is saved in a .h5 file in the specified output directory.\n",
    "\n",
    "Notebook content:\n",
    " - Determination of the threshold values for the Euclidean and Mahalanobis errors.\n",
    " - Model training.\n",
    " - Execution of the AtMonSat anomaly detection algorithm with the trained model on abnormal datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### User settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep-learning model to be used by the anomaly detection algorithm\n",
    "\n",
    "See the aux_notebooks/models.ipynb notebook for details on the individual architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CNN'\n",
    "#model_name = 'AutoEncoder'\n",
    "#model_name = 'LSTM'\n",
    "#model_name = 'LSTM_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of training epochs and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name of the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'results_2022.11.14'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal datasets to be used for training and threshold values determination.\n",
    "\n",
    "The experiment of 2022.07.20 contains only an abnormal dataset. Therefore, it is not on the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_datasets = [\"2022.04.06\", \"2022.05.18\", \"2022.05.20\", \"2022.05.30\", \"2022.06.01\",\n",
    "                \"2022.06.03\", \"2022.06.08\", \"2022.06.15\", \"2022.06.22\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print('Model:', model_name)\n",
    "print('Window length:', window_length)\n",
    "print('Number of epochs:', epochs)\n",
    "print('Training batch size:', batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Internal settings - not to be modified by the User "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_format = 'timestamp'\n",
    "\n",
    "DeltaLastChangeTimes_analysis = True\n",
    "\n",
    "if DeltaLastChangeTimes_analysis:\n",
    "    interpolation = False\n",
    "    if interpolation:\n",
    "        # Interpolation interval in seconds\n",
    "        time_interpolation_interval = 5\n",
    "\n",
    "normalise = True\n",
    "# The 'Delta Last Change Times' transformed data should not be normalised.\n",
    "normalise = normalise and (not DeltaLastChangeTimes_analysis)\n",
    "\n",
    "PCA_higher_order_analysis = False\n",
    "if PCA_higher_order_analysis:\n",
    "    kPCA = False\n",
    "    \n",
    "# For anomaly_times variable to be defined.\n",
    "anomaly_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "\n",
    "print(\"DeltaLastChangeTimes_analysis:\", DeltaLastChangeTimes_analysis)\n",
    "if DeltaLastChangeTimes_analysis:\n",
    "    print(\"\\tInterpolation:\", interpolation)\n",
    "    if interpolation:\n",
    "        print(\"\\t\\tTime interpolation interval:\", str(time_interpolation_interval) + \"s\")\n",
    "print(\"Normalise:\",normalise)\n",
    "print(\"PCA_higher_order_analysis:\", PCA_higher_order_analysis)\n",
    "if PCA_higher_order_analysis:\n",
    "    print('\\tKernel PCA:', kPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_COLOR = 'darkblue'\n",
    "TP_INTERVAL_COLOR = ANOMALY_COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISLC_implementation = ''\n",
    "#ISLC_implementation = 'uint32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MINMAXScaler or StandardScaler is now turned off!\n",
    "apply_scaling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path_common_prefix = 'datasets'\n",
    "normal_dataset_file = os.path.join('processed_data','normal_data_temperature.csv')\n",
    "abnormal_dataset_file = os.path.join('processed_data','anomalous_data_temperature.csv')\n",
    "\n",
    "anomalies = {}\n",
    "\n",
    "### Anomalies times\n",
    "anomalies['2022.04.06'] = ['06.04.2022 16:36:47.00','06.04.2022 17:21:30.00','06.04.2022 17:50:59.00']\n",
    "anomalies['2022.05.18'] = ['18.05.2022 16:14:24.00', '18.05.2022 16:56:19.00', '18.05.2022 17:44:32.00']\n",
    "anomalies['2022.05.20'] = ['20.05.2022 19:14:37.00']\n",
    "anomalies['2022.05.30'] = ['30.05.2022 18:26:24.00']\n",
    "anomalies['2022.06.01'] = ['01.06.2022 12:49:21.00', '01.06.2022 14:26:10.00', '01.06.2022 16:06:33.00',\n",
    "                           '01.06.2022 16:59:09.00']\n",
    "anomalies['2022.06.03'] = ['03.06.2022 11:57:08.00']\n",
    "anomalies['2022.06.08'] = ['08.06.2022 16:46:56.00', '08.06.2022 17:25:53.00']\n",
    "anomalies['2022.06.15'] = ['15.06.2022 15:05:00.00', '15.06.2022 15:46:29.00']\n",
    "anomalies['2022.06.22'] = ['22.06.2022 15:47:23.00']\n",
    "anomalies['2022.07.20'] = ['20.07.2022 11:58:35.00', '20.07.2022 12:31:39.00', '20.07.2022 12:39:49.00',\n",
    "                           '20.07.2022 14:38:26.00']\n",
    "\n",
    "# Abnormal datasets trimming\n",
    "# --------------------------\n",
    "#  - assuring the initial part of length window_size is without anomalies;\n",
    "#  - removing the end parts which contained noise due to experimental setup disassembly. \n",
    "\n",
    "new_anomalous_data_starts = {'2022.05.18':'18.05.2022 14:30:00.00', \n",
    "                             '2022.06.22':'22.06.2022 15:30:00.00'}\n",
    "\n",
    "new_anomalous_data_ends = {'2022.06.15':'15.06.2022 16:20:00.00',\n",
    "                           '2022.06.22':'22.06.2022 16:50:00.00'}\n",
    "\n",
    "abnormal_datasets_dates = anomalies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normal_features = [\"temp_{}\".format(i) for i in range(0,9)]\n",
    "abnormal_features = [\"temp_{}\".format(i) for i in range(0,9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preparatory actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class instantiation\n",
    "\n",
    "The class is be used to load the normal datasets for model trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_example = False\n",
    "\n",
    "%run dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    CHECK_FOLDER = os.path.isdir(output_folder)\n",
    "    \n",
    "    if not CHECK_FOLDER:\n",
    "        try:\n",
    "            os.makedirs(output_folder)\n",
    "            print(\"Directory '%s' created successfully\" % output_folder)\n",
    "            break\n",
    "        except OSError as error:\n",
    "            print(OSError)\n",
    "            break\n",
    "    else:\n",
    "        print(output_folder, \"directory already exists.\")\n",
    "        output_folder = input(\"Please provide a new directory name:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run aux_notebooks/function_library.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def parse(x):\n",
    "    return pd.to_datetime(x, unit='s', origin='unix').round('S')\n",
    "    \n",
    "def parse_modified(x):\n",
    "    return pd.to_datetime(x, origin='unix').round('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Determination of the anomaly detection thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_data = []\n",
    "mahalanobis_validation_data = []\n",
    "\n",
    "# Cross-validation\n",
    "for validation_dataset_date in use_datasets:\n",
    "        \n",
    "    print('Leave-one-out dataset:', validation_dataset_date)\n",
    "    \n",
    "    datasets_dates = use_datasets.copy()\n",
    "    datasets_dates.remove(validation_dataset_date)\n",
    "    print('Training datasets:', datasets_dates)\n",
    "    \n",
    "    datasets = []\n",
    "    for dataset in datasets_dates:\n",
    "        try:\n",
    "            datasets.append(Dataset(dataset).extract('normal'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    VALIDATION_DATA = os.path.join(data_path_common_prefix,validation_dataset_date,normal_dataset_file)\n",
    "    validation_df_full = pd.read_csv(VALIDATION_DATA,index_col=0,delimiter=',', parse_dates = ['timestamp'], date_parser=parse_modified)\n",
    "    validation_df = validation_df_full\n",
    "    df_valid_values = validation_df[normal_features].values\n",
    "    \n",
    "    %run aux_notebooks/error_thresholds_determination.ipynb\n",
    "    \n",
    "    validation_data.append(validation_error_data)\n",
    "    mahalanobis_validation_data.append(mahalanobis_validation_error_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot histrograms and determine the threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(x=flatten(validation_data), bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold = np.percentile(flatten(validation_data), 95)\n",
    "print('Euclidean error threshold value:', error_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = plt.rcParams.get('font.size')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "n, bins, patches = plt.hist(x=flatten(validation_data), bins='auto')\n",
    "plt.vlines(error_threshold,0,400,colors='r',linestyles='dashed',label='Significance threshold')\n",
    "plt.xlabel('Euclidean error')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=range(0,1500,200))\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder,'error_histogram.pdf'), format='pdf', bbox_inches='tight',\n",
    "            pad_inches=0.1)\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': fs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(x=flatten(mahalanobis_validation_data), bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mahalanobis_error_threshold = np.percentile(flatten(mahalanobis_validation_data), 99)\n",
    "mahalanobis_error_threshold = np.percentile(flatten(mahalanobis_validation_data), 95)\n",
    "print('Mahalanobis error threshold value:', mahalanobis_error_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for dataset in datasets_dates:\n",
    "    try:\n",
    "        datasets.append(Dataset(dataset).extract('normal'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Validation dataset for training\n",
    "VALIDATION_DATASET = os.path.join(data_path_common_prefix,\n",
    "                                  '2022.03.23','processed_data','validation_data_2022.03.23.csv')\n",
    "validation_df_full = pd.read_csv(VALIDATION_DATASET, index_col=0, delimiter=',',\n",
    "                                 parse_dates = ['timestamp'], date_parser=parse_modified)\n",
    "validation_df = validation_df_full\n",
    "\n",
    "%run aux_notebooks/train_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform post-processing of the Mahalanabis errors for the validation normal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The Mahalanobis errors are in m_dist computeted by aux_notebooks/train_model.ipynb.\n",
    "\n",
    "anomaly_inertia_window_length = 60\n",
    "\n",
    "# Consider a sequence of contiguous above-the-threshold values as a single anomaly detection signal \n",
    "# that indicates the occurance of the anomaly at the timepoint corresponding to the first\n",
    "# above-the-threshold value in the sequence.\n",
    "# Example: 0 0 0 1 1 1 0 0 1 1 0 -> 0 0 0 1 0 0 0 0 1 0 0\n",
    "# To consider: Not first above-the-threshold value, but the max above-the-threshold value in a sequence.\n",
    "    \n",
    "pp_anomaly_detection_signal = np.zeros(len(m_dist))\n",
    "pp_anomaly_signal_active = False\n",
    "\n",
    "for ind_entry, entry in enumerate(m_dist >= mahalanobis_error_threshold):\n",
    "        \n",
    "    if entry and (not pp_anomaly_signal_active):\n",
    "\n",
    "        pp_anomaly_detection_signal[ind_entry] = 1\n",
    "        pp_anomaly_signal_active = True\n",
    "\n",
    "    if pp_anomaly_signal_active and (not entry):\n",
    "        pp_anomaly_signal_active = False\n",
    "    \n",
    "# Remove secondary anomaly detection signals, i.e., signals that appear within the \n",
    "# anomaly_inertia_window_length after an primary anomaly.\n",
    "# Example: 0 0 0 1 0 0 1 0 0 1 0 0 0 -> 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
    "#                a - - - -    for anomaly_inertia_window_length = 4\n",
    "i = 0\n",
    "while i < len(pp_anomaly_detection_signal):\n",
    "        \n",
    "    if (pp_anomaly_detection_signal[i] == 1):\n",
    "            \n",
    "        j = 1\n",
    "        while (j <= anomaly_inertia_window_length) and (i+j < len(pp_anomaly_detection_signal)):\n",
    "            pp_anomaly_detection_signal[i+j] = 0\n",
    "            j = j + 1\n",
    "            \n",
    "        i = i + j\n",
    "        \n",
    "    else:\n",
    "        i = i + 1\n",
    "    \n",
    "print('Number of post-processed False Positives for the validation dataset:', sum(pp_anomaly_detection_signal))\n",
    "print('Number of post-processed True Negatives for the validation dataset:',\n",
    "      len(pp_anomaly_detection_signal) - sum(pp_anomaly_detection_signal) - window_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the output of the anomaly detection algorithm on the validation normal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "fs = plt.rcParams.get('font.size')\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots(figsize=(20, 2))\n",
    "if DeltaLastChangeTimes_analysis:\n",
    "    plt.plot(cet_valid, pp_anomaly_detection_signal, color='#16B9F5')\n",
    "else:\n",
    "    plt.plot(pp_anomaly_detection_signal, color='#16B9F5')\n",
    "plt.yticks(ticks=[0,1],labels=['Norm','Abnorm'])\n",
    "\n",
    "date_form = DateFormatter(\"%H:%M\")\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_formatter(date_form)\n",
    "plt.xlabel('Timestamp [HH:MM]',labelpad=20)\n",
    "\n",
    "plt.savefig(os.path.join(output_folder,'validation_anomaly_detection.pdf'), format='pdf',\n",
    "            bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': fs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving of the trained model and some variables to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "variables_dict = {'mahalanobis_mean':mean,\n",
    "                  'mahalanobis_matrix':inv_cov,\n",
    "                  'detection_threshold':mahalanobis_error_threshold,\n",
    "                  'Euclidean_threshold':error_threshold\n",
    "                 }\n",
    "\n",
    "with open(os.path.join(output_folder,'saved_variables.pkl'), 'wb') as file:\n",
    "    pickle.dump(variables_dict, file)\n",
    "\n",
    "model.save(os.path.join(output_folder,'trained_model.h5'), save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Loading of a trained model and some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#MODEL_FOLDER = './results'\n",
    "\n",
    "#with open(os.path.join(MODEL_FOLDER,'saved_variables.pkl'), 'rb') as file:\n",
    "#    variables = pickle.load(file)\n",
    "    \n",
    "#mean = variables['mahalanobis_mean']\n",
    "#mahalanobis_error_threshold = variables['detection_threshold']\n",
    "#cov = np.linalg.inv(variables['mahalanobis_matrix'])\n",
    "#error_threshold = variables['Euclidean_threshold']\n",
    "\n",
    "#model = keras.models.load_model(os.path.join(MODEL_FOLDER,'./trained_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The AtMonSat Anomaly Detection Algorithm\n",
    "\n",
    "Besides running the algorithm itself, the code provides quantitative evaluation of the algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Used for calculating raw and after post-processing TPs, FPs, TNs, and FNs.\n",
    "\n",
    "def getConfusionTableValues(df,features,t_anomalies,anomaly_inertia_window_length,threshold,make_plot=False):\n",
    "\n",
    "    ### Phase I of post-processing (only considered for pp_ prefixed values)\n",
    "    # Consider a sequence of contiguous above-the-threshold values as a single anomaly detection signal \n",
    "    # that indicates the occurance of the anomaly at the timepoint corresponding to the first\n",
    "    # above-the-threshold value in the sequence.\n",
    "    # Example: 0 0 0 1 1 1 0 0 1 1 0 -> 0 0 0 1 0 0 0 0 1 0 0\n",
    "    # To consider: Not first above-the-threshold value, but the max above-the-threshold value in a sequence.\n",
    "\n",
    "    anomaly_detection_signal = np.zeros(len(m_dist))\n",
    "    \n",
    "    pp_anomaly_detection_signal = np.zeros(len(m_dist))\n",
    "    pp_anomaly_signal_active = False\n",
    "\n",
    "    for ind_entry, entry in enumerate(m_dist >= threshold):\n",
    "\n",
    "        if entry:\n",
    "\n",
    "            anomaly_detection_signal[ind_entry] = 1\n",
    "        \n",
    "        if entry and (not pp_anomaly_signal_active):\n",
    "\n",
    "            pp_anomaly_detection_signal[ind_entry] = 1\n",
    "            pp_anomaly_signal_active = True\n",
    "\n",
    "        if pp_anomaly_signal_active and (not entry):\n",
    "            pp_anomaly_signal_active = False\n",
    "    \n",
    "    ### Phase II of post-processing (only considered for pp_ prefixed values)\n",
    "    # Remove secondary anomaly detection signals, i.e., signals that appear within the \n",
    "    # anomaly_inertia_window_length after an primary anomaly.\n",
    "    # Example: 0 0 0 1 0 0 1 0 0 1 0 0 0 -> 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
    "    #                a - - - -    for anomaly_inertia_window_length = 4\n",
    "    i = 0\n",
    "    while i < len(pp_anomaly_detection_signal):\n",
    "        \n",
    "        if (pp_anomaly_detection_signal[i] == 1):\n",
    "            \n",
    "            j = 1\n",
    "            while (j <= anomaly_inertia_window_length) and (i+j < len(pp_anomaly_detection_signal)):\n",
    "                pp_anomaly_detection_signal[i+j] = 0\n",
    "                j = j + 1\n",
    "            \n",
    "            i = i + j\n",
    "        \n",
    "        else:\n",
    "            i = i + 1\n",
    "        \n",
    "    output = np.zeros(len(anomaly_detection_signal))\n",
    "    pp_output = np.zeros(len(pp_anomaly_detection_signal))\n",
    "    # 0 - TN, 1 - TP, -1 - FP\n",
    "    \n",
    "    FN = 0\n",
    "    pp_FN = 0\n",
    "    \n",
    "    ### Count True Positives (TPs) and False Negatives (FNs):\n",
    "    \n",
    "    # Get a dictionary with t_anomalies as keys and pairs of anomaly index and the detection margin before\n",
    "    # anomaly.\n",
    "    interpInfo = getInterpolationInfoForAnomalies(df,features,t_anomalies)\n",
    "    \n",
    "    for t_anomaly in t_anomalies:\n",
    "        \n",
    "        # All anomaly detection signals in TP_interval should be considered as True Positives\n",
    "        TP_interval = range(interpInfo[t_anomaly][0]-interpInfo[t_anomaly][1],\n",
    "                            min(interpInfo[t_anomaly][0]+anomaly_inertia_window_length + 1,\n",
    "                                len(df.index))\n",
    "                           )\n",
    "        \n",
    "        anomaly_detected = False\n",
    "        for i in TP_interval:\n",
    "            if (anomaly_detection_signal[i] == 1):\n",
    "                output[i] = 1\n",
    "                anomaly_detected = True\n",
    "        # Failed to detect the anomaly; increase the number of FNs\n",
    "        if not anomaly_detected:\n",
    "            FN = FN + 1\n",
    "            \n",
    "        pp_anomaly_detected = False\n",
    "        for i in TP_interval:\n",
    "            if (pp_anomaly_detection_signal[i] == 1):\n",
    "                pp_output[i] = 1\n",
    "                pp_anomaly_detected = True\n",
    "        # Failed to detect the anomaly; increase the number of FNs\n",
    "        if not pp_anomaly_detected:\n",
    "            pp_FN = pp_FN + 1\n",
    "    \n",
    "    ### Count False Positives (FPs): \n",
    "    ### All signals in anomaly_detection_signal not tagged 1 in output are considered as FPs.\n",
    "    for i in range(len(anomaly_detection_signal)):\n",
    "        if (anomaly_detection_signal[i] == 1) and (output[i] == 0):\n",
    "            output[i] = -1\n",
    "            \n",
    "    TP = sum(output == 1)\n",
    "    FP = sum(output == -1)\n",
    "    TN = len(anomaly_detection_signal) - (TP + FP + FN)\n",
    "    \n",
    "    for i in range(len(pp_anomaly_detection_signal)):\n",
    "        if (pp_anomaly_detection_signal[i] == 1) and (pp_output[i] == 0):\n",
    "            pp_output[i] = -1\n",
    "            \n",
    "    pp_TP = sum(pp_output == 1)\n",
    "    pp_FP = sum(pp_output == -1)\n",
    "    pp_TN = len(pp_anomaly_detection_signal) - (pp_TP + pp_FP + pp_FN)\n",
    "    \n",
    "    if make_plot:\n",
    "        #figure(figsize=(20, 7))\n",
    "        #plt.vlines(t_anomalies,0,1,color='b')\n",
    "        #plt.plot(cet_abnorm, m_dist/max(m_dist), color='r')\n",
    "        #plt.plot(df.index,output,color='g')\n",
    "        #plt.show()\n",
    "        \n",
    "        fs = plt.rcParams.get('font.size')\n",
    "        plt.rcParams.update({'font.size': 20})\n",
    "        fig, ax = plt.subplots(figsize=(20, 2))\n",
    "        plt.plot(cet_abnorm, pp_anomaly_detection_signal, color='#16B9F5')\n",
    "        plt.vlines(t_anomalies,0,1,colors=ANOMALY_COLOR)\n",
    "        for t_anomaly in t_anomalies:\n",
    "            TP_interval = (interpInfo[t_anomaly][0]-interpInfo[t_anomaly][1],\n",
    "                           min(interpInfo[t_anomaly][0]+anomaly_inertia_window_length + 1,len(df.index))\n",
    "                          )\n",
    "            start_time = cet_abnorm[TP_interval[0]]\n",
    "            end_time = cet_abnorm[TP_interval[1]-1]\n",
    "            ax.fill_between(cet_abnorm, 0, 1, where= (start_time <= cet_abnorm) & (cet_abnorm <= end_time),\n",
    "                             facecolor=TP_INTERVAL_COLOR, alpha=0.15)\n",
    "        plt.yticks(ticks=[0,1],labels=['Norm','Abnorm'])\n",
    "        date_form = DateFormatter(\"%H:%M\")\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.xlabel('Timestamp [HH:MM]')\n",
    "        plt.savefig(os.path.join(output_folder, 'anomaly_detection_' + anomaly_ds + '.pdf'), format='pdf',\n",
    "                    bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.show()\n",
    "        plt.rcParams.update({'font.size': fs})\n",
    "    \n",
    "    return {'TP':TP, 'FP':FP, 'TN':TN, 'FN':FN,\n",
    "            'pp_TP':pp_TP, 'pp_FP':pp_FP, 'pp_TN':pp_TN, 'pp_FN':pp_FN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### For benchmark rule-based approach\n",
    "\n",
    "temperature_change_window = anomaly_inertia_window_length\n",
    "\n",
    "def checkTempIncrease(df):\n",
    "    \n",
    "    increased = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        increase_found = False\n",
    "        row = 0\n",
    "        while (row < df.shape[0]-1):\n",
    "            if (df[col].iloc[row] < df[col].iloc[row+1]):\n",
    "                increase_found = True\n",
    "            row = row + 1\n",
    "                \n",
    "        increased.append(increase_found)\n",
    "        \n",
    "    return np.array(increased).all()\n",
    "\n",
    "def getAnomalyIntervals(predictions,anomaly_inertia_window_length):   \n",
    "    predicted_anomaly_intervals = []\n",
    "\n",
    "    anomaly_signal_active = False\n",
    "    \n",
    "    new_interval_found = False\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        \n",
    "        if (predictions[i] == 1):\n",
    "\n",
    "            if not anomaly_signal_active:\n",
    "                start_ind = i\n",
    "                anomaly_signal_active = True\n",
    "\n",
    "            if (i == len(predictions) - 1):\n",
    "                end_ind = i\n",
    "                new_interval_found = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            if anomaly_signal_active:\n",
    "                end_ind = i - 1\n",
    "                new_interval_found = True\n",
    "                anomaly_signal_active = False\n",
    "                \n",
    "        if new_interval_found:\n",
    "            \n",
    "            while (end_ind - start_ind + 1 > 2 * anomaly_inertia_window_length):\n",
    "                \n",
    "                predicted_anomaly_intervals.append((start_ind,start_ind + 2 * anomaly_inertia_window_length - 1))\n",
    "                start_ind = start_ind + 2 * anomaly_inertia_window_length\n",
    "                \n",
    "            predicted_anomaly_intervals.append((start_ind,end_ind))\n",
    "            \n",
    "            new_interval_found = False\n",
    "            \n",
    "    return predicted_anomaly_intervals\n",
    "    \n",
    "# Test\n",
    "# getAnomalyIntervals(np.array([0,0,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1]),3)\n",
    "\n",
    "def getBenchmarkConfusionTableValues(predictions,anomaly_inertia_window_length,df,t_anomalies):\n",
    "    \n",
    "    ais = getAnomalyIntervals(predictions,anomaly_inertia_window_length)\n",
    "    \n",
    "    ind_anomalies = [list(df.index >= t_anomaly).index(True) for t_anomaly in t_anomalies]\n",
    "    \n",
    "    # It is assumed that the indices of the anomalies are monotonically increasing\n",
    "    assert(all(ind_anomalies[i] <= ind_anomalies[i+1] for i in range(len(ind_anomalies) - 1)))\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    # i is the index iterating over the ind_anomalies list, which contains the positions (indices) of the\n",
    "    # predictions array at which the anomalies were introduced.\n",
    "    i = 0\n",
    "    for interval in ais:\n",
    "        \n",
    "        # All anomalies in ind_anomalies were considered and/or there are no more anomalies to be detected.\n",
    "        # Therefore, all the remaining intervals in ais are FPs.\n",
    "        if (i == len(ind_anomalies)):\n",
    "            \n",
    "            FP = FP + 1\n",
    "        \n",
    "        while i < len(ind_anomalies):\n",
    "            \n",
    "            # The current interval starts after the currently considered anomaly and all the remaining\n",
    "            # intervals start even later. Therefore, the current anomaly is not detected and the number\n",
    "            # of FNs is increased.\n",
    "            if (interval[0] > ind_anomalies[i]):\n",
    "                \n",
    "                FN = FN + 1\n",
    "                i = i + 1\n",
    "            \n",
    "            # The current interval ends before the occurence of the currently considered anomaly and it\n",
    "            # does not detect any of the previous anomalies. Therefore, it is a FP. i is not increased,\n",
    "            # as one of the next intervals may be a detector of this anomaly - jump out of the while\n",
    "            # loop and consider the next interval.\n",
    "            if (interval[1] < ind_anomalies[i]):\n",
    "\n",
    "                FP = FP + 1\n",
    "                break\n",
    "            \n",
    "            # The currently considered anomaly lies within the current interval. This interval is a TP.\n",
    "            if (interval[0] <= ind_anomalies[i]) and (ind_anomalies[i] <= interval[1]):\n",
    "                \n",
    "                TP = TP + 1\n",
    "                i = i + 1\n",
    "                \n",
    "                # More than one anomaly could be correctly detected within the current interval. This\n",
    "                # case is handled by the while loop.\n",
    "                if (i == len(ind_anomalies)):\n",
    "                    break\n",
    "                else:\n",
    "                    while (interval[0] <= ind_anomalies[i]) and (ind_anomalies[i] <= interval[1]):\n",
    "                        TP = TP + 1\n",
    "                        i = i + 1\n",
    "                        if (i == len(ind_anomalies)):\n",
    "                            break\n",
    "\n",
    "                # Jump out of the while loop and condsider both the next interval and the next anomaly\n",
    "                # (if exists).\n",
    "                break\n",
    "\n",
    "    # No intervals for the remaining anomalies, which are undetected and contribute to the number of FNs.\n",
    "    while i < len(ind_anomalies):\n",
    "        \n",
    "        FN = FN + 1\n",
    "        i = i + 1 \n",
    "        \n",
    "    TN = len(predictions)\n",
    "    for interval in ais:\n",
    "        TN = TN - (interval[1] - interval[0] + 1)\n",
    "\n",
    "    return {'TP':TP, 'FP':FP, 'TN':TN, 'FN':FN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for anomaly_ds in abnormal_datasets_dates:\n",
    "    \n",
    "    print('Anomalous dataset:', anomaly_ds)\n",
    "    \n",
    "    ABNORMAL_DATA = os.path.join(data_path_common_prefix,anomaly_ds,abnormal_dataset_file)\n",
    "    abnormal_df_full = pd.read_csv(ABNORMAL_DATA, index_col=0, delimiter=',',\n",
    "                                   parse_dates = ['timestamp'], date_parser=parse_modified)\n",
    "    abnormal_df = abnormal_df_full\n",
    "        \n",
    "    ### Apply trimming if necessary.\n",
    "    new_start = new_anomalous_data_starts.get(anomaly_ds)\n",
    "    new_end = new_anomalous_data_ends.get(anomaly_ds)\n",
    "\n",
    "    if not new_start is None:\n",
    "        t_new_start = datetime.strptime(new_start, '%d.%m.%Y %H:%M:%S.%f')\n",
    "        t_new_start_ind = list(abnormal_df.index >= t_new_start).index(True)\n",
    "        abnormal_df = abnormal_df.iloc[t_new_start_ind:,:]\n",
    "        \n",
    "    if not new_end is None:\n",
    "        t_new_end = datetime.strptime(new_end, '%d.%m.%Y %H:%M:%S.%f')\n",
    "        t_new_end_ind = list(abnormal_df.index > t_new_end).index(True)\n",
    "        abnormal_df = abnormal_df.iloc[:t_new_end_ind,:]\n",
    "    \n",
    "    anomaly_times = anomalies[anomaly_ds]\n",
    "    t_anomalies = [datetime.strptime(anomaly_time, '%d.%m.%Y %H:%M:%S.%f') for anomaly_time in anomaly_times]\n",
    "    \n",
    "    %run aux_notebooks/detect_anomalies.ipynb\n",
    "    \n",
    "    confusionTableDict = getConfusionTableValues(abnormal_df,abnormal_features,t_anomalies,\n",
    "                                                 anomaly_inertia_window_length,mahalanobis_error_threshold,\n",
    "                                                 make_plot=True\n",
    "                                                )\n",
    "    print('Confusion table for the Mahalanobis error threshold:', confusionTableDict)\n",
    "    \n",
    "    if (confusionTableDict['TP'] == 0):\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        F1score = 0\n",
    "    else:\n",
    "        precision = confusionTableDict['TP']/(confusionTableDict['TP'] + confusionTableDict['FP'])\n",
    "        recall = confusionTableDict['TP']/(confusionTableDict['TP'] + confusionTableDict['FN'])\n",
    "        F1score = 2*(precision * recall) / (precision + recall)\n",
    "\n",
    "    if (confusionTableDict['pp_TP'] == 0):\n",
    "        pp_precision = 0\n",
    "        pp_recall = 0\n",
    "        pp_F1score = 0\n",
    "    else:\n",
    "        pp_precision = confusionTableDict['pp_TP']/(confusionTableDict['pp_TP'] + confusionTableDict['pp_FP'])\n",
    "        pp_recall = confusionTableDict['pp_TP']/(confusionTableDict['pp_TP'] + confusionTableDict['pp_FN'])\n",
    "        pp_F1score = 2*(pp_precision * pp_recall) / (pp_precision + pp_recall)\n",
    "    \n",
    "    print('Precision:', precision, '| Recall:', recall, '| F1 score:', F1score)\n",
    "    print('pp_Precision:', pp_precision, '| pp_Recall:', pp_recall, '| pp_F1 score:', pp_F1score)\n",
    "    \n",
    "    # Thresholds are sorted in the increasing order by np.unique() and then reverted. In result,\n",
    "    # the thresholds are sorted in the decreasing order.\n",
    "    #\n",
    "    # As in sklearn.metrics.roc_curve(), thresholds[0] represents no instances being predicted\n",
    "    # and is arbitrarily set to max(m_dist) + 1. This is to make sure that the curve starts at (0, 0).\n",
    "    thresholds = np.append(np.unique(m_dist), max(m_dist)+1)[::-1]\n",
    "    assert(all(thresholds[i] >= thresholds[i+1] for i in range(len(thresholds) - 1)))\n",
    "\n",
    "    FPRs = []\n",
    "    TPRs = []\n",
    "\n",
    "    pp_FPRs = []\n",
    "    pp_TPRs = []\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    pp_precisions = []\n",
    "    pp_recalls = []\n",
    "\n",
    "    #for thr in tqdm(thresholds):\n",
    "    for i in tqdm(range(len(thresholds))):\n",
    "\n",
    "        thr = thresholds[i]\n",
    "\n",
    "        #confusionTableDict = getConfusionTableValues(abnormal_df,abnormal_features,t_anomalies,\n",
    "        #                                             anomaly_inertia_window_length,thr)\n",
    "\n",
    "        confusionTableDict = getConfusionTableValues(abnormal_df,abnormal_features,t_anomalies,\n",
    "                                                     anomaly_inertia_window_length,thr)\n",
    "\n",
    "        pp_TP = confusionTableDict['pp_TP']\n",
    "        pp_FN = confusionTableDict['pp_FN']\n",
    "        pp_FP = confusionTableDict['pp_FP']\n",
    "        pp_TN = confusionTableDict['pp_TN']\n",
    "\n",
    "        pp_fpr = pp_FP/(pp_FP+pp_TN)\n",
    "        pp_tpr = pp_TP/(pp_TP+pp_FN)\n",
    "\n",
    "        pp_FPRs.append(pp_fpr)\n",
    "        pp_TPRs.append(pp_tpr)\n",
    "\n",
    "        # The first threshold value, i.e., max(m_dist)+1, is used only for the ROC curve.\n",
    "        if (i != 0):\n",
    "            pp_precision = pp_TP/(pp_TP + pp_FP)\n",
    "            pp_recall = pp_TP/(pp_TP + pp_FN)\n",
    "\n",
    "            pp_precisions.append(pp_precision)\n",
    "            pp_recalls.append(pp_recall)\n",
    "\n",
    "        TP = confusionTableDict['TP']\n",
    "        FN = confusionTableDict['FN']\n",
    "        FP = confusionTableDict['FP']\n",
    "        TN = confusionTableDict['TN']\n",
    "\n",
    "        fpr = FP/(FP+TN)\n",
    "        tpr = TP/(TP+FN)\n",
    "\n",
    "        FPRs.append(fpr)\n",
    "        TPRs.append(tpr)\n",
    "\n",
    "        # The first threshold value, i.e., max(m_dist)+1, is used only for the ROC curve.\n",
    "        if (i != 0):\n",
    "            precision = TP/(TP + FP)\n",
    "            recall = TP/(TP + FN)\n",
    "\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "\n",
    "    # Reverse the precisions and recalls so recalls is decreasing.\n",
    "    #\n",
    "    # As in the case of sklearn.metrics.precision_recall_curve(), the last precision and recall values\n",
    "    # are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph\n",
    "    # starts on the y axis.\n",
    "\n",
    "    sl = slice(None, None, -1)\n",
    "    precisions = precisions[sl]\n",
    "    precisions.append(1)\n",
    "    recalls = recalls[sl]\n",
    "    recalls.append(0)\n",
    "\n",
    "    assert(all(recalls[i] >= recalls[i+1] for i in range(len(recalls) - 1)))\n",
    "\n",
    "    pp_precisions = pp_precisions[sl]\n",
    "    pp_precisions.append(1)\n",
    "    pp_recalls = pp_recalls[sl]\n",
    "    pp_recalls.append(0)\n",
    "\n",
    "    roc_auc = auc(FPRs, TPRs)\n",
    "\n",
    "    pr_auc = auc(recalls, precisions)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        FPRs,\n",
    "        TPRs,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    #plt.title(\"Receiver operating characteristic curve\")\n",
    "    plt.legend(loc=\"lower right\",fontsize=16)\n",
    "    \n",
    "    plot_file_name = os.path.join(output_folder,'ROC_curve_' + anomaly_ds + '.pdf')\n",
    "    plt.savefig(plot_file_name, format='pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n",
    "    # Both the precision and the recall are focused on the positive class (the minority class) and are \n",
    "    # unconcerned with the true negatives (majority class).\n",
    "    #\n",
    "    #    \"… precision and recall make it possible to assess the performance of a classifier\n",
    "    #     on the minority class.\"\n",
    "    #\n",
    "    #       — Page 27, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n",
    "    #\n",
    "    #    \"Precision-recall curves (PR curves) are recommended for highly skewed domains where\n",
    "    #     ROC curves may provide an excessively optimistic view of the performance.\"\n",
    "    #\n",
    "    #       — A Survey of Predictive Modelling under Imbalanced Distributions, 2015.\n",
    "\n",
    "    # calculate the no skill line as the proportion of the positive class\n",
    "    no_skill = len(t_anomalies)/len(m_dist)\n",
    "    # plot the no skill precision-recall curve\n",
    "    fs = plt.rcParams.get('font.size')\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(recalls, precisions, marker='.', label=\"Precision-Recall curve (area = %0.2f)\" % pr_auc)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    plot_file_name = os.path.join(output_folder,'Precision-Recall_curve_' + anomaly_ds + '.pdf')\n",
    "    plt.savefig(plot_file_name, format='pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "  \n",
    "    plt.show()\n",
    "    plt.rcParams.update({'font.size': fs})\n",
    "    \n",
    "    ### Benchmark rule-based method for comparison\n",
    "    ### ==========================================\n",
    "    \n",
    "    print(\"========= Benchmark rule-based method =========\")\n",
    "    \n",
    "    anomaly_detection = []\n",
    "    for i in range(abnormal_df.shape[0]-temperature_change_window):\n",
    "        if checkTempIncrease(abnormal_df[abnormal_features].iloc[i:i+temperature_change_window]):\n",
    "            anomaly_detection.append(1)\n",
    "        else:\n",
    "            anomaly_detection.append(0)\n",
    "\n",
    "    for i in range(temperature_change_window):\n",
    "        if checkTempIncrease(abnormal_df[abnormal_features].iloc[abnormal_df.shape[0]-temperature_change_window+i:]):\n",
    "            anomaly_detection.append(1)\n",
    "        else:\n",
    "            anomaly_detection.append(0)\n",
    "\n",
    "    fs = plt.rcParams.get('font.size')\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    figure(figsize=(20, 2))\n",
    "    plt.plot(abnormal_df.index,anomaly_detection, color='#16B9F5')\n",
    "    plt.vlines(t_anomalies,0,1,colors=ANOMALY_COLOR)\n",
    "    plt.yticks(ticks=[0,1],labels=['Norm','Abnorm'])\n",
    "    date_form = DateFormatter(\"%H:%M\")\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(date_form)\n",
    "    plt.xlabel('Timestamp [HH:MM]',labelpad=20)\n",
    "    \n",
    "    plot_file_name = os.path.join(output_folder,'benchmark_anomaly_detection_' + anomaly_ds + '.pdf')\n",
    "    plt.savefig(plot_file_name, format='pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.rcParams.update({'font.size': fs})\n",
    "    \n",
    "    print(\"Confusion matrix of the benchmark rule-based method:\")\n",
    "    bm_confusionTableDict = getBenchmarkConfusionTableValues(anomaly_detection,temperature_change_window,abnormal_df,t_anomalies)\n",
    "    print(bm_confusionTableDict)\n",
    "    \n",
    "    if (bm_confusionTableDict['TP'] == 0):\n",
    "        bm_precision = 0\n",
    "        bm_recall = 0\n",
    "        bm_F1score = 0\n",
    "    else:\n",
    "        bm_precision = bm_confusionTableDict['TP']/(bm_confusionTableDict['TP'] + bm_confusionTableDict['FP'])\n",
    "        bm_recall = bm_confusionTableDict['TP']/(bm_confusionTableDict['TP'] + bm_confusionTableDict['FN'])\n",
    "        bm_F1score = 2*(bm_precision * bm_recall) / (bm_precision + bm_recall)\n",
    "    \n",
    "    print('Benchmark precision:', bm_precision, '| Benchmark recall:', bm_recall,\n",
    "          '| Benchmark F1 score:', bm_F1score)\n",
    "    \n",
    "    print(\"====================================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
