{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copyright 2022 University of Luxembourg\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "> you may not use this file except in compliance with the License.  \n",
    "> You may obtain a copy of the License at  \n",
    ">\n",
    ">    https://www.apache.org/licenses/LICENSE-2.0\n",
    ">\n",
    "> Unless required by applicable law or agreed to in writing, software  \n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "> See the License for the specific language governing permissions and  \n",
    "> limitations under the License.  \n",
    ">\n",
    "***\n",
    "\n",
    "Author: Andrzej Mizera (andrzej.mizera@uni.lu)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Called to train the deep-learning model for the AtMonSat anomaly detection algorithm with the use of all normal datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "islc_norm_list = []\n",
    "df_values_list = []\n",
    "feature_norm_values_list = []\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    islc_norm = getIterationsSinceLastChangeMicro(ds.dataframe,normal_features)\n",
    "    islc_norm_list.append(islc_norm)\n",
    "    \n",
    "    df_values = pd.DataFrame(islc_norm)\n",
    "    df_values_list.append(df_values)\n",
    "    \n",
    "    feature_norm_values = ds.dataframe[normal_features].astype('float32').values\n",
    "    feature_norm_values_list.append(feature_norm_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "islc_valid = getIterationsSinceLastChangeMicro(validation_df,normal_features)\n",
    "df_valid_values = pd.DataFrame(islc_valid)\n",
    "feature_valid_values = validation_df[normal_features].astype('float32').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Preparation of the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del X, XX\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (model_name == 'CNN') or (model_name == 'LSTM') or (model_name == 'LSTM_2') or (model_name == 'AutoEncoder'):\n",
    "\n",
    "    for i in range(len(df_values_list)):\n",
    "        df_values = df_values_list[i]\n",
    "        temps_normal_values = feature_norm_values_list[i]\n",
    "        \n",
    "        if (model_name == 'AutoEncoder'):\n",
    "            n_features_in, n_features_out, X_part, XX_part = create_subseq_AE(np.append(df_values,temps_normal_values,axis=1), window_length)\n",
    "        else:\n",
    "            n_features_in, n_features_out, X_part, XX_part = create_subseq(np.append(df_values,temps_normal_values,axis=1), np.append(df_values,temps_normal_values,axis=1), window_length, 1)\n",
    "            \n",
    "        try:\n",
    "            X = np.append(X,X_part,axis=0)\n",
    "            XX = np.append(XX,XX_part,axis=0)\n",
    "        except NameError:\n",
    "            X = X_part\n",
    "            XX = XX_part\n",
    "\n",
    "    if (model_name == 'AutoEncoder'):\n",
    "        _, _, X_valid, Y_valid = create_subseq_AE(np.append(df_valid_values,feature_valid_values,axis=1), window_length)\n",
    "    else:\n",
    "        _, _, X_valid, Y_valid = create_subseq(np.append(df_valid_values,feature_valid_values,axis=1), np.append(df_valid_values,feature_valid_values,axis=1), window_length, 1)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Wrong model name!')\n",
    "    \n",
    "print('Model type:', model_name)\n",
    "print('Number of input features:', n_features_in)\n",
    "print('Number of output features:', n_features_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model construction and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta=1e-2, patience=10, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run aux_notebooks/models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranining of other models than single-channel LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Keras TensorBoard callback.\n",
    "#logdir=os.path.join(output_folder,logs,fit,datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "### Choose which loss function to use ###\n",
    "### --------------------------------- ###\n",
    "model.compile(loss=\"mse\",optimizer='adam')\n",
    "#model.compile(loss=loss_fun,optimizer='adam')\n",
    "    \n",
    "model.build(input_shape=(None, window_length, n_features_in))\n",
    "model.summary()\n",
    "\n",
    "#cbs = [early_stop,tensorboard_callback]\n",
    "cbs = [early_stop]\n",
    "    \n",
    "history=model.fit(x=X, y=np.squeeze(XX), \n",
    "                  validation_data=(np.array(X_valid), np.squeeze(np.array(Y_valid))),\n",
    "                  epochs=epochs, batch_size=batch_size, shuffle=True, callbacks=[early_stop]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "#%load_ext tensorboard\n",
    "\n",
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of the Gaussian distribution on the train errors for Mahalanobis distance computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of the prediction errors on the training data subset, i.e., train errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X)\n",
    "\n",
    "# If only one time point ahead is predicted, XX is of dimension (num_points, 1, n_features). However, pred is\n",
    "# then of dimension (num_points, n_features). Therefore, the shape of pred needs to be expanded:\n",
    "pred_np = pred.numpy()\n",
    "if (len(pred_np.shape) == 2):\n",
    "    pred_np = np.expand_dims(pred_np,axis=1)\n",
    "\n",
    "te = pred_np - XX\n",
    "\n",
    "train_errors = np.reshape(te,(te.shape[0],te.shape[1]*te.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sum(train_errors)/len(train_errors)\n",
    "\n",
    "cov = 0\n",
    "for e in train_errors:\n",
    "    cov += np.dot((e-mean).reshape(len(e), 1), (e-mean).reshape(1, len(e)))\n",
    "cov /= len(train_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## For validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the prediction errors on the valdation data, i.e., validation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cet_valid = validation_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(np.array(X_valid))\n",
    "    \n",
    "pred_np = pred.numpy()\n",
    "if (len(pred_np.shape) == 2):\n",
    "    pred_np = np.expand_dims(pred_np,axis=1)\n",
    "\n",
    "te = pred_np - np.array(Y_valid)\n",
    "\n",
    "valid_errors = np.reshape(te,(te.shape[0],te.shape[1]*te.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = np.zeros(window_length)\n",
    "\n",
    "def score(x):\n",
    "    if not kPCA:\n",
    "        pca_variances = pca.explained_variance_[first_higher_order_PCA:]\n",
    "    else:\n",
    "        pca_variances = pca.lambdas_[first_higher_order_PCA:]\n",
    "\n",
    "    return sum(np.divide(np.square(x),pca_variances))\n",
    "\n",
    "def vec_length(x):\n",
    "    return np.sqrt(sum(np.square(x)))\n",
    "\n",
    "if PCA_higher_order_analysis:\n",
    "    error_fun = score\n",
    "else:\n",
    "    error_fun = vec_length\n",
    "\n",
    "figure(figsize=(20, 7))\n",
    "if DeltaLastChangeTimes_analysis:\n",
    "    plt.plot(cet_valid,\n",
    "             np.concatenate((padding,np.apply_along_axis(error_fun, 1, valid_errors))),\n",
    "             color='r', label='Euclidean error'\n",
    "            )\n",
    "else:\n",
    "    plt.plot(np.concatenate((padding,np.apply_along_axis(error_fun, 1, valid_errors))),\n",
    "             color='r', label='Euclidean error'\n",
    "            )\n",
    "\n",
    "plt.title('Prediction errors for the validation dataset')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Euclidean error')\n",
    "\n",
    "if DeltaLastChangeTimes_analysis:\n",
    "    plt.hlines(error_threshold,cet_valid[0],cet_valid[-1],'g','dashed',label='Euclidean error threshold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Mahalanobis distances for the prediction errors on the test data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Mahalanobis distance\n",
    "def Mahala_distance(x,mean,cov):\n",
    "    d = np.dot(x-mean,np.linalg.inv(cov))\n",
    "    d = np.dot(d, (x-mean).T)\n",
    "    return d\n",
    "\n",
    "m_dist = [0]*window_length \n",
    "for e in valid_errors:\n",
    "    m_dist.append(Mahala_distance(e,mean,cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Mahalanobis distance for the test dataset predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "fs = plt.rcParams.get('font.size')\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "figure(figsize=(20, 7))\n",
    "if DeltaLastChangeTimes_analysis:\n",
    "    plt.plot(cet_valid,m_dist, color='r', label='Mahalanobis error')\n",
    "else:\n",
    "    plt.plot(m_dist, color='r', label='Mahalanobis error')\n",
    "\n",
    "if DeltaLastChangeTimes_analysis:\n",
    "    plt.hlines(mahalanobis_error_threshold,cet_valid[0],cet_valid[-1],'g','dashed',\n",
    "               label='Mahalanobis error threshold')\n",
    "\n",
    "date_form = DateFormatter(\"%H:%M\")\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_formatter(date_form)\n",
    "plt.xlabel('Timestamp [HH:MM]',labelpad=20)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(output_folder,'validation_mahalanobis_error.pdf'), format='pdf',\n",
    "            bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': fs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of False Positives for the validation dataset:', sum(m_dist >= mahalanobis_error_threshold))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
